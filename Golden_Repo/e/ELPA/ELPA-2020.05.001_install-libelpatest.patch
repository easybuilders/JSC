--- elpa-2020.05.001/Makefile.am	2020-06-15 14:39:19.000000000 +0200
+++ elpa-2020.05.001_ok/Makefile.am	2020-06-25 09:15:40.115746908 +0200
@@ -498,7 +498,7 @@
 test_program_fcflags = $(AM_FCFLAGS) $(FC_MODOUT)test_modules $(FC_MODINC)test_modules $(FC_MODINC)modules $(FC_MODINC)private_modules
 
 # library with shared sources for the test files
-noinst_LTLIBRARIES += libelpatest@SUFFIX@.la
+lib_LTLIBRARIES += libelpatest@SUFFIX@.la
 libelpatest@SUFFIX@_la_FCFLAGS = $(test_program_fcflags)
 libelpatest@SUFFIX@_la_SOURCES = \
   test/shared/tests_variable_definitions.F90 \
diff -ruN elpa-2020.05.001/examples/C/Makefile_examples_hybrid elpa-2020.05.001_ok/examples/C/Makefile_examples_hybrid
--- elpa-2020.05.001/examples/C/Makefile_examples_hybrid	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/Makefile_examples_hybrid	2020-06-25 09:35:05.174752000 +0200
@@ -0,0 +1,21 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES_OPENMP) -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE_OPENMP)/elpa -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB_OPENMP) -lelpa_openmp -lelpatest_openmp $(SCALAPACK_LIB) $(MKL)
+CC            = mpicc -O3 
+
+all: test_real_1stage_hybrid test_real_2stage_all_kernels_hybrid test_autotune_hybrid test_multiple_objs_hybrid
+
+test_real_1stage_hybrid: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DWITH_OPENMP -DTEST_EIGENVECTORS -DWITH_MPI -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_real_2stage_all_kernels_hybrid: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DWITH_OPENMP -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_MPI -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_autotune_hybrid: test_autotune.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DWITH_OPENMP -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE) -o $@ test_autotune.c $(LIBS)
+
+test_multiple_objs_hybrid: test_multiple_objs.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DWITH_OPENMP -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE) -o $@ test_multiple_objs.c $(LIBS)
+
diff -ruN elpa-2020.05.001/examples/C/Makefile_examples_pure elpa-2020.05.001_ok/examples/C/Makefile_examples_pure
--- elpa-2020.05.001/examples/C/Makefile_examples_pure	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/Makefile_examples_pure	2020-06-25 09:31:37.807696000 +0200
@@ -0,0 +1,21 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL)
+CC            = mpicc -O3 
+
+all: test_real_1stage test_real_2stage_all_kernels test_autotune test_multiple_objs
+
+test_real_1stage: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DTEST_EIGENVECTORS -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_real_2stage_all_kernels: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_autotune: test_autotune.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test_autotune.c $(LIBS)
+
+test_multiple_objs: test_multiple_objs.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test_multiple_objs.c $(LIBS)
+
diff -ruN elpa-2020.05.001/examples/C/Makefile_examples_pure_cuda elpa-2020.05.001_ok/examples/C/Makefile_examples_pure_cuda
--- elpa-2020.05.001/examples/C/Makefile_examples_pure_cuda	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/Makefile_examples_pure_cuda	2020-06-25 09:33:39.050944000 +0200
@@ -0,0 +1,21 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL)
+CC            = mpicc -O3 
+
+all: test_real_1stage test_real_2stage_all_kernels test_autotune test_multiple_objs
+
+test_real_1stage: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DTEST_EIGENVECTORS -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_real_2stage_all_kernels: test.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test.c $(LIBS)
+
+test_autotune: test_autotune.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test_autotune.c $(LIBS)
+
+test_multiple_objs: test_multiple_objs.c
+	$(CC) -DCURRENT_API_VERSION=20190524 -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -I$(ELPA_INCLUDE) -o $@ test_multiple_objs.c $(LIBS)
+
diff -ruN elpa-2020.05.001/examples/C/test_autotune.c elpa-2020.05.001_ok/examples/C/test_autotune.c
--- elpa-2020.05.001/examples/C/test_autotune.c	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/test_autotune.c	2020-06-25 09:21:55.801360000 +0200
@@ -0,0 +1,335 @@
+/*   This file is part of ELPA.
+
+     The ELPA library was originally created by the ELPA consortium,
+     consisting of the following organizations:
+
+     - Max Planck Computing and Data Facility (MPCDF), formerly known as
+       Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+     - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+       Informatik,
+     - Technische Universität München, Lehrstuhl für Informatik mit
+       Schwerpunkt Wissenschaftliches Rechnen ,
+     - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+     - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+       Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+       and
+     - IBM Deutschland GmbH
+
+
+     More information can be found here:
+     http://elpa.mpcdf.mpg.de/
+
+     ELPA is free software: you can redistribute it and/or modify
+     it under the terms of the version 3 of the license of the
+     GNU Lesser General Public License as published by the Free
+     Software Foundation.
+
+     ELPA is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU Lesser General Public License for more details.
+
+     You should have received a copy of the GNU Lesser General Public License
+     along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+
+     ELPA reflects a substantial effort on the part of the original
+     ELPA consortium, and we ask you to respect the spirit of the
+     license that we chose: i.e., please contribute any changes you
+     may have back to the original ELPA library distribution, and keep
+     any derivatives of ELPA under the same license that we chose for
+     the original distribution, the GNU Lesser General Public License.
+*/
+
+#include "config.h"
+
+#include <string.h>
+#include <stdio.h>
+#include <stdlib.h>
+#ifdef WITH_MPI
+#include <mpi.h>
+#endif
+#include <math.h>
+
+#include <elpa/elpa.h>
+#include <assert.h>
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+//#error "define exactly one of TEST_REAL or TEST_COMPLEX"
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+//#error "define exactly one of TEST_SINGLE or TEST_DOUBLE"
+#endif
+
+#if !(defined(TEST_SOLVER_1STAGE) ^ defined(TEST_SOLVER_2STAGE))
+//#error "define exactly one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE"
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE float
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE float
+#  else
+#    define MATRIX_TYPE complex float
+#  endif
+#else
+#  define EV_TYPE double
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE double
+#  else
+#    define MATRIX_TYPE complex double
+#  endif
+#endif
+
+#define assert_elpa_ok(x) assert(x == ELPA_OK)
+
+#ifdef HAVE_64BIT_INTEGER_SUPPORT
+#define TEST_C_INT_TYPE_PTR long int*
+#define C_INT_TYPE_PTR long int*
+#define TEST_C_INT_TYPE long int
+#define C_INT_TYPE long int
+#else
+#define TEST_C_INT_TYPE_PTR int*
+#define C_INT_TYPE_PTR int*
+#define TEST_C_INT_TYPE int
+#define C_INT_TYPE int
+#endif
+
+#include "test/shared/generated.h"
+
+int main(int argc, char** argv) {
+   /* matrix dimensions */
+   C_INT_TYPE na, nev, nblk;
+
+   /* mpi */
+   C_INT_TYPE myid, nprocs;
+   C_INT_TYPE na_cols, na_rows;
+   C_INT_TYPE np_cols, np_rows;
+   C_INT_TYPE my_prow, my_pcol;
+   C_INT_TYPE mpi_comm;
+
+   /* blacs */
+   C_INT_TYPE my_blacs_ctxt, sc_desc[9], info;
+
+   /* The Matrix */
+   MATRIX_TYPE *a, *as, *z;
+   EV_TYPE *ev;
+
+   C_INT_TYPE status;
+   int error_elpa; 
+   elpa_t handle;
+
+   elpa_autotune_t autotune_handle;
+   C_INT_TYPE i, unfinished;
+
+   C_INT_TYPE value;
+#ifdef WITH_MPI
+   MPI_Init(&argc, &argv);
+   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
+   MPI_Comm_rank(MPI_COMM_WORLD, &myid);
+#else
+   nprocs = 1;
+   myid = 0;
+#endif
+
+   if (argc == 4) {
+     na = atoi(argv[1]);
+     nev = atoi(argv[2]);
+     nblk = atoi(argv[3]);
+   } else {
+     na = 500;
+     nev = 250;
+     nblk = 16;
+   }
+
+   for (np_cols = (C_INT_TYPE) sqrt((double) nprocs); np_cols > 1; np_cols--) {
+     if (nprocs % np_cols == 0) {
+       break;
+     }
+   }
+
+   np_rows = nprocs/np_cols;
+
+   /* set up blacs */
+   /* convert communicators before */
+#ifdef WITH_MPI
+   mpi_comm = MPI_Comm_c2f(MPI_COMM_WORLD);
+#else
+   mpi_comm = 0;
+#endif
+   set_up_blacsgrid_f(mpi_comm, np_rows, np_cols, 'C', &my_blacs_ctxt, &my_prow, &my_pcol);
+   set_up_blacs_descriptor_f(na, nblk, my_prow, my_pcol, np_rows, np_cols, &na_rows, &na_cols, sc_desc, my_blacs_ctxt, &info);
+
+   /* allocate the matrices needed for elpa */
+   a  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   z  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   as = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   ev = calloc(na, sizeof(EV_TYPE));
+
+#ifdef TEST_REAL
+#ifdef TEST_DOUBLE
+   prepare_matrix_random_real_double_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#else
+   prepare_matrix_random_real_single_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#endif
+#else
+#ifdef TEST_DOUBLE
+   prepare_matrix_random_complex_double_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#else
+   prepare_matrix_random_complex_single_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#endif
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) != ELPA_OK) {
+     fprintf(stderr, "Error: ELPA API version not supported");
+     exit(1);
+   }
+
+#ifdef OPTIONAL_C_ERROR_ARGUMENT
+   handle = elpa_allocate();
+#else
+   handle = elpa_allocate(&error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+   assert_elpa_ok(error_elpa);
+
+   /* Set parameters */
+   elpa_set(handle, "na", (int) na, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "nev", (int) nev, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   if (myid == 0) {
+     printf("Setting the matrix parameters na=%d, nev=%d \n",na,nev);
+   }
+   elpa_set(handle, "local_nrows", (int) na_rows, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "local_ncols", (int) na_cols, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "nblk", (int) nblk, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+#ifdef WITH_MPI
+   elpa_set(handle, "mpi_comm_parent", (int) (MPI_Comm_c2f(MPI_COMM_WORLD)), &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "process_row", (int) my_prow, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "process_col", (int) my_pcol, &error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+
+   /* Setup */
+   assert_elpa_ok(elpa_setup(handle));
+
+   elpa_set(handle, "gpu", 0, &error_elpa);
+   assert_elpa_ok(error_elpa);
+ 
+   autotune_handle = elpa_autotune_setup(handle, ELPA_AUTOTUNE_FAST, ELPA_AUTOTUNE_DOMAIN_REAL, &error_elpa);
+   assert_elpa_ok(error_elpa);
+   /* mimic 20 scf steps */
+
+   for (i=0; i < 20; i++) {
+
+      unfinished = elpa_autotune_step(handle, autotune_handle, &error_elpa);
+
+      if (unfinished == 0) {
+        if (myid == 0) {
+       	  printf("ELPA autotuning finished in the %d th scf step \n",i);
+        }
+	break;
+      }
+      if (myid == 0) {
+	printf("The current setting of the ELPA object: \n");
+        elpa_print_settings(handle, &error_elpa);
+
+	printf("The state of the autotuning: \n");
+        elpa_autotune_print_state(handle, autotune_handle, &error_elpa);
+      }
+
+
+      /* Solve EV problem */
+      elpa_eigenvectors(handle, a, ev, z, &error_elpa);
+      assert_elpa_ok(error_elpa);
+
+      /* check the results */
+#ifdef TEST_REAL
+#ifdef TEST_DOUBLE
+      status = check_correctness_evp_numeric_residuals_real_double_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(double));
+
+#else
+      status = check_correctness_evp_numeric_residuals_real_single_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(float));
+#endif
+#else
+#ifdef TEST_DOUBLE
+      status = check_correctness_evp_numeric_residuals_complex_double_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(complex double));
+#else
+      status = check_correctness_evp_numeric_residuals_complex_single_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(complex float));
+#endif
+#endif
+
+      if (status !=0){
+        printf("The computed EVs are not correct !\n");
+	break;
+      }
+      printf("hier %d \n",myid);
+   }
+
+   if (unfinished == 1) {
+     if (myid == 0) {
+        printf("ELPA autotuning did not finished during %d scf cycles\n",i);
+
+     }	     
+
+   }
+   elpa_autotune_set_best(handle, autotune_handle, &error_elpa);
+
+   if (myid == 0) {
+     printf("The best combination found by the autotuning:\n");
+     elpa_autotune_print_best(handle, autotune_handle, &error_elpa);
+   }
+
+#ifdef OPTIONAL_C_ERROR_ARGUMENT
+   elpa_autotune_deallocate(autotune_handle);
+   elpa_deallocate(handle);
+#else
+   elpa_autotune_deallocate(autotune_handle, &error_elpa);
+   elpa_deallocate(handle, &error_elpa);
+#endif
+   elpa_uninit(&error_elpa);
+
+   if (myid == 0) {
+     printf("\n");
+     printf("2stage ELPA real solver complete\n");
+     printf("\n");
+   }
+
+   if (status ==0){
+     if (myid ==0) {
+       printf("All ok!\n");
+     }
+   }
+
+   free(a);
+   free(z);
+   free(as);
+   free(ev);
+
+#ifdef WITH_MPI
+   MPI_Finalize();
+#endif
+
+   return !!status;
+}
diff -ruN elpa-2020.05.001/examples/C/test.c elpa-2020.05.001_ok/examples/C/test.c
--- elpa-2020.05.001/examples/C/test.c	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/test.c	2020-06-25 09:21:55.793130000 +0200
@@ -0,0 +1,339 @@
+/*   This file is part of ELPA.
+
+     The ELPA library was originally created by the ELPA consortium,
+     consisting of the following organizations:
+
+     - Max Planck Computing and Data Facility (MPCDF), formerly known as
+       Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+     - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+       Informatik,
+     - Technische Universität München, Lehrstuhl für Informatik mit
+       Schwerpunkt Wissenschaftliches Rechnen ,
+     - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+     - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+       Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+       and
+     - IBM Deutschland GmbH
+
+
+     More information can be found here:
+     http://elpa.mpcdf.mpg.de/
+
+     ELPA is free software: you can redistribute it and/or modify
+     it under the terms of the version 3 of the license of the
+     GNU Lesser General Public License as published by the Free
+     Software Foundation.
+
+     ELPA is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU Lesser General Public License for more details.
+
+     You should have received a copy of the GNU Lesser General Public License
+     along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+
+     ELPA reflects a substantial effort on the part of the original
+     ELPA consortium, and we ask you to respect the spirit of the
+     license that we chose: i.e., please contribute any changes you
+     may have back to the original ELPA library distribution, and keep
+     any derivatives of ELPA under the same license that we chose for
+     the original distribution, the GNU Lesser General Public License.
+*/
+
+#include "config.h"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#ifdef WITH_MPI
+#include <mpi.h>
+#endif
+#include <math.h>
+
+#include <elpa/elpa.h>
+#include <assert.h>
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+#error "define exactly one of TEST_REAL or TEST_COMPLEX"
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+#error "define exactly one of TEST_SINGLE or TEST_DOUBLE"
+#endif
+
+#if !(defined(TEST_SOLVER_1STAGE) ^ defined(TEST_SOLVER_2STAGE))
+#error "define exactly one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE"
+#endif
+
+#ifdef TEST_GENERALIZED_DECOMP_EIGENPROBLEM
+#define TEST_GENERALIZED_EIGENPROBLEM
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE float
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE float
+#    define PREPARE_MATRIX_RANDOM prepare_matrix_random_real_single_f
+#    define PREPARE_MATRIX_RANDOM_SPD prepare_matrix_random_spd_real_single_f
+#    define CHECK_CORRECTNESS_EVP_NUMERIC_RESIDUALS check_correctness_evp_numeric_residuals_real_single_f
+#    define CHECK_CORRECTNESS_EVP_GEN_NUMERIC_RESIDUALS check_correctness_evp_gen_numeric_residuals_real_single_f
+#  else
+#    define MATRIX_TYPE complex float
+#    define PREPARE_MATRIX_RANDOM prepare_matrix_random_complex_single_f
+#    define PREPARE_MATRIX_RANDOM_SPD prepare_matrix_random_spd_complex_single_f
+#    define CHECK_CORRECTNESS_EVP_NUMERIC_RESIDUALS check_correctness_evp_numeric_residuals_complex_single_f
+#    define CHECK_CORRECTNESS_EVP_GEN_NUMERIC_RESIDUALS check_correctness_evp_gen_numeric_residuals_complex_single_f
+#  endif
+#else
+#  define EV_TYPE double
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE double
+#    define PREPARE_MATRIX_RANDOM prepare_matrix_random_real_double_f
+#    define PREPARE_MATRIX_RANDOM_SPD prepare_matrix_random_spd_real_double_f
+#    define CHECK_CORRECTNESS_EVP_NUMERIC_RESIDUALS check_correctness_evp_numeric_residuals_real_double_f
+#    define CHECK_CORRECTNESS_EVP_GEN_NUMERIC_RESIDUALS check_correctness_evp_gen_numeric_residuals_real_double_f
+#  else
+#    define MATRIX_TYPE complex double
+#    define PREPARE_MATRIX_RANDOM prepare_matrix_random_complex_double_f
+#    define PREPARE_MATRIX_RANDOM_SPD prepare_matrix_random_spd_complex_double_f
+#    define CHECK_CORRECTNESS_EVP_NUMERIC_RESIDUALS check_correctness_evp_numeric_residuals_complex_double_f
+#    define CHECK_CORRECTNESS_EVP_GEN_NUMERIC_RESIDUALS check_correctness_evp_gen_numeric_residuals_complex_double_f
+#  endif
+#endif
+
+#define assert_elpa_ok(x) assert(x == ELPA_OK)
+
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_C_INT_TYPE_PTR long int*
+#define C_INT_TYPE_PTR long int*
+#define TEST_C_INT_TYPE long int
+#define C_INT_TYPE long int
+#else
+#define TEST_C_INT_TYPE_PTR int*
+#define C_INT_TYPE_PTR int*
+#define TEST_C_INT_TYPE int
+#define C_INT_TYPE int
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_C_INT_MPI_TYPE_PTR long int*
+#define C_INT_MPI_TYPE_PTR long int*
+#define TEST_C_INT_MPI_TYPE long int
+#define C_INT_MPI_TYPE long int
+#else
+#define TEST_C_INT_MPI_TYPE_PTR int*
+#define C_INT_MPI_TYPE_PTR int*
+#define TEST_C_INT_MPI_TYPE int
+#define C_INT_MPI_TYPE int
+#endif
+#include "test/shared/generated.h"
+
+int main(int argc, char** argv) {
+   /* matrix dimensions */
+   C_INT_TYPE na, nev, nblk;
+
+   /* mpi */
+   C_INT_TYPE myid, nprocs;
+   C_INT_MPI_TYPE myidMPI, nprocsMPI;
+   C_INT_TYPE na_cols, na_rows;
+   C_INT_TYPE np_cols, np_rows;
+   C_INT_TYPE my_prow, my_pcol;
+   C_INT_TYPE mpi_comm;
+   C_INT_MPI_TYPE provided_mpi_thread_level;
+
+   /* blacs */
+   C_INT_TYPE my_blacs_ctxt, sc_desc[9], info;
+
+   /* The Matrix */
+   MATRIX_TYPE *a, *as, *z, *b, *bs;
+   EV_TYPE *ev;
+
+   C_INT_TYPE error, status;
+   int error_elpa;
+
+   elpa_t handle;
+
+   int  value;
+#ifdef WITH_MPI
+#ifndef WITH_OPENMP
+   MPI_Init(&argc, &argv);
+#else
+   MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided_mpi_thread_level);
+
+   if (provided_mpi_thread_level != MPI_THREAD_MULTIPLE) {
+     fprintf(stderr, "MPI ERROR: MPI_THREAD_MULTIPLE is not provided on this system\n");
+     MPI_Finalize();
+     exit(77);
+   }
+#endif
+
+   MPI_Comm_size(MPI_COMM_WORLD, &nprocsMPI);
+   nprocs = (C_INT_TYPE) nprocsMPI;
+   MPI_Comm_rank(MPI_COMM_WORLD, &myidMPI);
+   myid = (C_INT_TYPE) myidMPI;
+
+#else
+   nprocs = 1;
+   myid = 0;
+#endif
+
+   if (argc == 4) {
+     na = atoi(argv[1]);
+     nev = atoi(argv[2]);
+     nblk = atoi(argv[3]);
+   } else {
+     na = 500;
+     nev = 250;
+     nblk = 16;
+   }
+
+   for (np_cols = (C_INT_TYPE) sqrt((double) nprocs); np_cols > 1; np_cols--) {
+     if (nprocs % np_cols == 0) {
+       break;
+     }
+   }
+
+   np_rows = nprocs/np_cols;
+
+   /* set up blacs */
+   /* convert communicators before */
+#ifdef WITH_MPI
+   mpi_comm = MPI_Comm_c2f(MPI_COMM_WORLD);
+#else
+   mpi_comm = 0;
+#endif
+   set_up_blacsgrid_f(mpi_comm, np_rows, np_cols, 'C', &my_blacs_ctxt, &my_prow, &my_pcol);
+   set_up_blacs_descriptor_f(na, nblk, my_prow, my_pcol, np_rows, np_cols, &na_rows, &na_cols, sc_desc, my_blacs_ctxt, &info);
+
+   /* allocate the matrices needed for elpa */
+   a  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   z  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   as = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   ev = calloc(na, sizeof(EV_TYPE));
+
+   PREPARE_MATRIX_RANDOM(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+   b  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   bs = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   PREPARE_MATRIX_RANDOM_SPD(na, myid, na_rows, na_cols, sc_desc, b, z, bs, nblk, np_rows, np_cols, my_prow, my_pcol);
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) != ELPA_OK) {
+     fprintf(stderr, "Error: ELPA API version not supported");
+     exit(1);
+   }
+
+   handle = elpa_allocate(&error_elpa);
+   //assert_elpa_ok(error_elpa);
+
+   /* Set parameters */
+   elpa_set(handle, "na", (int) na, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "nev", (int) nev, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   if (myid == 0) {
+     printf("Setting the matrix parameters na=%d, nev=%d \n",na,nev);
+   }
+   elpa_set(handle, "local_nrows", (int) na_rows, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "local_ncols", (int) na_cols, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "nblk", (int) nblk, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+#ifdef WITH_MPI
+   elpa_set(handle, "mpi_comm_parent", (int) (MPI_Comm_c2f(MPI_COMM_WORLD)), &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "process_row", (int) my_prow, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "process_col", (int) my_pcol, &error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+#ifdef TEST_GENERALIZED_EIGENPROBLEM
+   elpa_set(handle, "blacs_context", (int) my_blacs_ctxt, &error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+
+   /* Setup */
+   assert_elpa_ok(elpa_setup(handle));
+
+   /* Set tunables */
+#ifdef TEST_SOLVER_1STAGE
+   elpa_set(handle, "solver", ELPA_SOLVER_1STAGE, &error_elpa);
+#else
+   elpa_set(handle, "solver", ELPA_SOLVER_2STAGE, &error_elpa);
+#endif
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(handle, "gpu", TEST_GPU, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+#if defined(TEST_SOLVE_2STAGE) && defined(TEST_KERNEL)
+# ifdef TEST_COMPLEX
+   elpa_set(handle, "complex_kernel", TEST_KERNEL, &error_elpa);
+# else
+   elpa_set(handle, "real_kernel", TEST_KERNEL, &error_elpa);
+# endif
+   assert_elpa_ok(error_elpa);
+#endif
+
+   elpa_get(handle, "solver", &value, &error_elpa);
+   if (myid == 0) {
+     printf("Solver is set to %d \n", value);
+   }
+
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+     elpa_generalized_eigenvectors(handle, a, b, ev, z, 0, &error_elpa);
+#if defined(TEST_GENERALIZED_DECOMP_EIGENPROBLEM)
+     //a = as, so that the problem can be solved again
+     memcpy(a, as, na_rows * na_cols * sizeof(MATRIX_TYPE));
+     elpa_generalized_eigenvectors(handle, a, b, ev, z, 1, &error_elpa);
+#endif
+#else
+   /* Solve EV problem */
+   elpa_eigenvectors(handle, a, ev, z, &error_elpa);
+#endif
+   assert_elpa_ok(error_elpa);
+
+   elpa_deallocate(handle, &error_elpa);
+   elpa_uninit(&error_elpa);
+
+   /* check the results */
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+   status = CHECK_CORRECTNESS_EVP_GEN_NUMERIC_RESIDUALS(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol, bs);
+#else
+   status = CHECK_CORRECTNESS_EVP_NUMERIC_RESIDUALS(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+#endif
+
+   if (status !=0){
+     printf("The computed EVs are not correct !\n");
+   }
+   if (status ==0){
+     printf("All ok!\n");
+   }
+
+   free(a);
+   free(z);
+   free(as);
+   free(ev);
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+   free(b);
+   free(bs);
+#endif
+
+#ifdef WITH_MPI
+   MPI_Finalize();
+#endif
+
+   return !!status;
+}
diff -ruN elpa-2020.05.001/examples/C/test_multiple_objs.c elpa-2020.05.001_ok/examples/C/test_multiple_objs.c
--- elpa-2020.05.001/examples/C/test_multiple_objs.c	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/C/test_multiple_objs.c	2020-06-25 09:21:55.797454000 +0200
@@ -0,0 +1,387 @@
+/*   This file is part of ELPA.
+
+     The ELPA library was originally created by the ELPA consortium,
+     consisting of the following organizations:
+
+     - Max Planck Computing and Data Facility (MPCDF), formerly known as
+       Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+     - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+       Informatik,
+     - Technische Universität München, Lehrstuhl für Informatik mit
+       Schwerpunkt Wissenschaftliches Rechnen ,
+     - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+     - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+       Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+       and
+     - IBM Deutschland GmbH
+
+
+     More information can be found here:
+     http://elpa.mpcdf.mpg.de/
+
+     ELPA is free software: you can redistribute it and/or modify
+     it under the terms of the version 3 of the license of the
+     GNU Lesser General Public License as published by the Free
+     Software Foundation.
+
+     ELPA is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU Lesser General Public License for more details.
+
+     You should have received a copy of the GNU Lesser General Public License
+     along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+
+     ELPA reflects a substantial effort on the part of the original
+     ELPA consortium, and we ask you to respect the spirit of the
+     license that we chose: i.e., please contribute any changes you
+     may have back to the original ELPA library distribution, and keep
+     any derivatives of ELPA under the same license that we chose for
+     the original distribution, the GNU Lesser General Public License.
+*/
+
+#include "config.h"
+
+#include <string.h>
+#include <stdio.h>
+#include <stdlib.h>
+#ifdef WITH_MPI
+#include <mpi.h>
+#endif
+#include <math.h>
+
+#include <elpa/elpa.h>
+#include <assert.h>
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+//#error "define exactly one of TEST_REAL or TEST_COMPLEX"
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+//#error "define exactly one of TEST_SINGLE or TEST_DOUBLE"
+#endif
+
+#if !(defined(TEST_SOLVER_1STAGE) ^ defined(TEST_SOLVER_2STAGE))
+//#error "define exactly one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE"
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE float
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE float
+#  else
+#    define MATRIX_TYPE complex float
+#  endif
+#else
+#  define EV_TYPE double
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE double
+#  else
+#    define MATRIX_TYPE complex double
+#  endif
+#endif
+
+#define assert_elpa_ok(x) assert(x == ELPA_OK)
+#ifdef HAVE_64BIT_INTEGER_SUPPORT
+#define TEST_C_INT_TYPE_PTR long int*
+#define C_INT_TYPE_PTR long int*
+#define TEST_C_INT_TYPE long int
+#define C_INT_TYPE long int
+#else
+#define TEST_C_INT_TYPE_PTR int*
+#define C_INT_TYPE_PTR int*
+#define TEST_C_INT_TYPE int
+#define C_INT_TYPE int
+#endif
+
+#include "test/shared/generated.h"
+void set_basic_parameters(elpa_t *handle, C_INT_TYPE na, C_INT_TYPE nev, C_INT_TYPE na_rows, C_INT_TYPE na_cols, C_INT_TYPE nblk, C_INT_TYPE my_prow, C_INT_TYPE my_pcol){
+   int error_elpa;
+   elpa_set(*handle, "na", (int) na, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "nev", (int) nev, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "local_nrows", (int) na_rows, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "local_ncols", (int) na_cols, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "nblk", (int) nblk, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+#ifdef WITH_MPI
+   elpa_set(*handle, "mpi_comm_parent", (int) (MPI_Comm_c2f(MPI_COMM_WORLD)), &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "process_row", (int) my_prow, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(*handle, "process_col", (int) my_pcol, &error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+}
+
+
+int main(int argc, char** argv) {
+   /* matrix dimensions */
+   C_INT_TYPE na, nev, nblk;
+
+   /* mpi */
+   C_INT_TYPE myid, nprocs;
+   C_INT_TYPE na_cols, na_rows;
+   C_INT_TYPE np_cols, np_rows;
+   C_INT_TYPE my_prow, my_pcol;
+   C_INT_TYPE mpi_comm;
+
+   /* blacs */
+   C_INT_TYPE my_blacs_ctxt, sc_desc[9], info;
+
+   /* The Matrix */
+   MATRIX_TYPE *a, *as, *z;
+   EV_TYPE *ev;
+
+   C_INT_TYPE status;
+   int error_elpa;
+   int gpu, timings, debug; 
+   char str[400];
+
+   elpa_t elpa_handle_1, elpa_handle_2, *elpa_handle_ptr;
+
+   elpa_autotune_t autotune_handle;
+   C_INT_TYPE i, unfinished;
+
+   C_INT_TYPE value;
+#ifdef WITH_MPI
+   MPI_Init(&argc, &argv);
+   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
+   MPI_Comm_rank(MPI_COMM_WORLD, &myid);
+#else
+   nprocs = 1;
+   myid = 0;
+#endif
+
+   if (argc == 4) {
+     na = atoi(argv[1]);
+     nev = atoi(argv[2]);
+     nblk = atoi(argv[3]);
+   } else {
+     na = 500;
+     nev = 250;
+     nblk = 16;
+   }
+
+   for (np_cols = (C_INT_TYPE) sqrt((double) nprocs); np_cols > 1; np_cols--) {
+     if (nprocs % np_cols == 0) {
+       break;
+     }
+   }
+
+   np_rows = nprocs/np_cols;
+
+   /* set up blacs */
+   /* convert communicators before */
+#ifdef WITH_MPI
+   mpi_comm = MPI_Comm_c2f(MPI_COMM_WORLD);
+#else
+   mpi_comm = 0;
+#endif
+   set_up_blacsgrid_f(mpi_comm, np_rows, np_cols, 'C', &my_blacs_ctxt, &my_prow, &my_pcol);
+   set_up_blacs_descriptor_f(na, nblk, my_prow, my_pcol, np_rows, np_cols, &na_rows, &na_cols, sc_desc, my_blacs_ctxt, &info);
+
+   /* allocate the matrices needed for elpa */
+   a  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   z  = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   as = calloc(na_rows*na_cols, sizeof(MATRIX_TYPE));
+   ev = calloc(na, sizeof(EV_TYPE));
+
+#ifdef TEST_REAL
+#ifdef TEST_DOUBLE
+   prepare_matrix_random_real_double_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#else
+   prepare_matrix_random_real_single_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#endif
+#else
+#ifdef TEST_DOUBLE
+   prepare_matrix_random_complex_double_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#else
+   prepare_matrix_random_complex_single_f(na, myid, na_rows, na_cols, sc_desc, a, z, as);
+#endif
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) != ELPA_OK) {
+     fprintf(stderr, "Error: ELPA API version not supported");
+     exit(1);
+   }
+
+   elpa_handle_1 = elpa_allocate(&error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   set_basic_parameters(&elpa_handle_1, na, nev, na_rows, na_cols, nblk, my_prow, my_pcol);
+   /* Setup */
+   assert_elpa_ok(elpa_setup(elpa_handle_1));
+
+   elpa_set(elpa_handle_1, "gpu", 0, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(elpa_handle_1, "timings", 1, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_set(elpa_handle_1, "debug", 1, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_store_settings(elpa_handle_1, "initial_parameters.txt", &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+#ifdef WITH_MPI
+     // barrier after store settings, file created from one MPI rank only, but loaded everywhere
+     MPI_Barrier(MPI_COMM_WORLD);
+#endif
+
+#ifdef OPTIONAL_C_ERROR_ARGUMENT
+   elpa_handle_2 = elpa_allocate();
+#else
+   elpa_handle_2 = elpa_allocate(&error_elpa);
+   assert_elpa_ok(error_elpa);
+#endif
+
+   set_basic_parameters(&elpa_handle_2, na, nev, na_rows, na_cols, nblk, my_prow, my_pcol);
+   /* Setup */
+   assert_elpa_ok(elpa_setup(elpa_handle_2));
+
+   elpa_load_settings(elpa_handle_2, "initial_parameters.txt", &error_elpa);
+
+   elpa_get(elpa_handle_2, "gpu", &gpu, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_get(elpa_handle_2, "timings", &timings, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   elpa_get(elpa_handle_2, "debug", &debug, &error_elpa);
+   assert_elpa_ok(error_elpa);
+
+   if ((timings != 1) || (debug != 1) || (gpu != 0)){
+     printf("Parameters not stored or loaded correctly. Aborting... %d, %d, %d\n", timings, debug, gpu);
+     exit(1);
+   }
+
+   elpa_handle_ptr = &elpa_handle_2;
+
+   autotune_handle = elpa_autotune_setup(*elpa_handle_ptr, ELPA_AUTOTUNE_FAST, ELPA_AUTOTUNE_DOMAIN_REAL, &error_elpa);
+   assert_elpa_ok(error_elpa);
+   /* mimic 20 scf steps */
+
+   for (i=0; i < 20; i++) {
+
+      unfinished = elpa_autotune_step(*elpa_handle_ptr, autotune_handle, &error_elpa);
+
+      if (unfinished == 0) {
+        if (myid == 0) {
+          printf("ELPA autotuning finished in the %d th scf step \n",i);
+        }
+        break;
+      }
+
+      elpa_print_settings(*elpa_handle_ptr, &error_elpa);
+      elpa_autotune_print_state(*elpa_handle_ptr, autotune_handle, &error_elpa);
+
+      sprintf(str, "saved_parameters_%d.txt", i);
+      elpa_store_settings(*elpa_handle_ptr, str, &error_elpa);
+      assert_elpa_ok(error_elpa);
+
+      /* Solve EV problem */
+      elpa_eigenvectors(*elpa_handle_ptr, a, ev, z, &error_elpa);
+      assert_elpa_ok(error_elpa);
+
+      /* check the results */
+#ifdef TEST_REAL
+#ifdef TEST_DOUBLE
+      status = check_correctness_evp_numeric_residuals_real_double_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(double));
+
+#else
+      status = check_correctness_evp_numeric_residuals_real_single_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(float));
+#endif
+#else
+#ifdef TEST_DOUBLE
+      status = check_correctness_evp_numeric_residuals_complex_double_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(complex double));
+#else
+      status = check_correctness_evp_numeric_residuals_complex_single_f(na, nev, na_rows, na_cols, as, z, ev,
+                                sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol);
+      memcpy(a, as, na_rows*na_cols*sizeof(complex float));
+#endif
+#endif
+
+      if (status !=0){
+        printf("The computed EVs are not correct !\n");
+        break;
+      }
+
+     elpa_autotune_print_state(*elpa_handle_ptr, autotune_handle, &error_elpa);
+     assert_elpa_ok(error_elpa);
+
+     sprintf(str, "saved_state_%d.txt", i);
+     elpa_autotune_save_state(*elpa_handle_ptr, autotune_handle, str, &error_elpa);
+     assert_elpa_ok(error_elpa);
+
+#ifdef WITH_MPI
+     //barrier after save state, file created from one MPI rank only, but loaded everywhere
+     MPI_Barrier(MPI_COMM_WORLD);
+#endif
+
+     elpa_autotune_load_state(*elpa_handle_ptr, autotune_handle, str, &error_elpa);
+     assert_elpa_ok(error_elpa);
+
+     if (unfinished == 1) {
+       if (myid == 0) {
+          printf("ELPA autotuning did not finished during %d scf cycles\n",i);
+       }
+     }
+
+   }
+   elpa_autotune_set_best(*elpa_handle_ptr, autotune_handle, &error_elpa);
+
+   if (myid == 0) {
+     printf("The best combination found by the autotuning:\n");
+     elpa_autotune_print_best(*elpa_handle_ptr, autotune_handle, &error_elpa);
+   }
+
+   elpa_autotune_deallocate(autotune_handle, &error_elpa);
+   elpa_deallocate(elpa_handle_1, &error_elpa);
+#ifdef OPTIONAL_C_ERROR_ARGUMENT
+   elpa_deallocate(elpa_handle_2);
+#else
+   elpa_deallocate(elpa_handle_2, &error_elpa);
+#endif
+   elpa_uninit(&error_elpa);
+
+   if (myid == 0) {
+     printf("\n");
+     printf("2stage ELPA real solver complete\n");
+     printf("\n");
+   }
+
+   if (status ==0){
+     if (myid ==0) {
+       printf("All ok!\n");
+     }
+   }
+
+   free(a);
+   free(z);
+   free(as);
+   free(ev);
+
+#ifdef WITH_MPI
+   MPI_Finalize();
+#endif
+
+   return !!status;
+}
diff -ruN elpa-2020.05.001/examples/Fortran/assert.h elpa-2020.05.001_ok/examples/Fortran/assert.h
--- elpa-2020.05.001/examples/Fortran/assert.h	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/assert.h	2020-06-25 09:22:05.752530000 +0200
@@ -0,0 +1,7 @@
+#define stringify_(x) "x"
+#define stringify(x) stringify_(x)
+#define assert(x) call x_a(x, stringify(x), "F", __LINE__)
+
+#define assert_elpa_ok(error_code) call x_ao(error_code, stringify(error_code), __FILE__, __LINE__)
+
+! vim: syntax=fortran
diff -ruN elpa-2020.05.001/examples/Fortran/elpa2/complex_2stage_banded.F90 elpa-2020.05.001_ok/examples/Fortran/elpa2/complex_2stage_banded.F90
--- elpa-2020.05.001/examples/Fortran/elpa2/complex_2stage_banded.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa2/complex_2stage_banded.F90	2020-06-25 09:22:05.794030000 +0200
@@ -0,0 +1,295 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+#include "../assert.h"
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 2 complex case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (500, 150, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+!> The complex ELPA 2 kernel is set as the default kernel.
+!> However, this can be overriden by setting
+!> the environment variable "COMPLEX_ELPA_KERNEL" to an
+!> appropiate value.
+!>
+
+#include "config-f90.h"
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+
+program test_complex2_double_banded
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - COMPLEX version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!-------------------------------------------------------------------------------
+   use elpa
+
+   !use test_util
+   use test_read_input_parameters
+   use test_check_correctness
+   use test_setup_mpi
+   use test_blacs_infrastructure
+   use test_prepare_matrix
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   use test_output_type
+   implicit none
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   TEST_INT_TYPE              :: nblk
+   TEST_INT_TYPE              :: na, nev
+
+   TEST_INT_TYPE              :: np_rows, np_cols, na_rows, na_cols
+
+   TEST_INT_TYPE              :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   TEST_INT_TYPE              :: i, my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+   TEST_INT_MPI_TYPE          :: mpierr
+#ifdef WITH_MPI
+   !TEST_INT_TYPE, external    :: numroc
+#endif
+   complex(kind=ck8), parameter   :: CZERO = (0.0_rk8,0.0_rk8), CONE = (1.0_rk8,0.0_rk8)
+   real(kind=rk8), allocatable    :: ev(:)
+
+   complex(kind=ck8), allocatable :: a(:,:), z(:,:), as(:,:)
+
+   TEST_INT_TYPE              :: STATUS
+#ifdef WITH_OPENMP
+   TEST_INT_TYPE              :: omp_get_max_threads,  required_mpi_thread_level, provided_mpi_thread_level
+#endif
+   type(output_t)                :: write_to_file
+   integer(kind=c_int)          :: error_elpa
+   character(len=8)              :: task_suffix
+   TEST_INT_TYPE              :: j
+
+
+   TEST_INT_TYPE              :: numberOfDevices
+   TEST_INT_TYPE              :: global_row, global_col, local_row, local_col
+   TEST_INT_TYPE              :: bandwidth
+   class(elpa_t), pointer        :: e
+
+#define COMPLEXCASE
+#define DOUBLE_PRECISION_COMPLEX 1
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+      !-------------------------------------------------------------------------------
+   !  MPI Initialization
+   call setup_mpi(myid, nprocs)
+
+   STATUS = 0
+
+   !-------------------------------------------------------------------------------
+   ! Selection of number of processor rows/columns
+   ! We try to set up the grid square-like, i.e. start the search for possible
+   ! divisors of nprocs with a number next to the square root of nprocs
+   ! and decrement it until a divisor is found.
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   if(myid==0) then
+      print *
+      print '(a)','Standard eigenvalue problem - COMPLEX version'
+      print *
+      print '(3(a,i0))','Matrix size=',na,', Number of eigenvectors=',nev,', Block size=',nblk
+      print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+      print *
+   endif
+
+   !-------------------------------------------------------------------------------
+   ! Set up BLACS context and MPI communicators
+   !
+   ! The BLACS context is only necessary for using Scalapack.
+   !
+   ! For ELPA, the MPI communicators along rows/cols are sufficient,
+   ! and the grid setup may be done in an arbitrary way as long as it is
+   ! consistent (i.e. 0<=my_prow<np_rows, 0<=my_pcol<np_cols and every
+   ! process has a unique (my_prow,my_pcol) pair).
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, 'C', &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+
+   ! Determine the necessary size of the distributed matrices,
+   ! we use the Scalapack tools routine NUMROC for that.
+
+   call set_up_blacs_descriptor(na ,nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+   !-------------------------------------------------------------------------------
+   ! Allocate matrices and set up a test matrix for the eigenvalue problem
+
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+
+   allocate(ev(na))
+
+   call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+
+   ! set values outside of the bandwidth to zero
+   bandwidth = nblk
+
+   do local_row = 1, na_rows
+     global_row = index_l2g( local_row, nblk, my_prow, np_rows )
+     do local_col = 1, na_cols
+       global_col = index_l2g( local_col, nblk, my_pcol, np_cols )
+
+       if (ABS(global_row-global_col) > bandwidth) then
+         a(local_row, local_col) = 0
+         as(local_row, local_col) = 0
+       end if
+     end do
+   end do
+
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   call e%set("bandwidth", int(bandwidth,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   assert(e%setup() .eq. ELPA_OK)
+
+   call e%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+   call elpa_deallocate(e, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   !-------------------------------------------------------------------------------
+   ! Test correctness of result (using plain scalapack routines)
+   status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+
+   deallocate(a)
+   deallocate(as)
+
+   deallocate(z)
+   deallocate(ev)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call EXIT(STATUS)
+end
+
+!-------------------------------------------------------------------------------
diff -ruN elpa-2020.05.001/examples/Fortran/elpa2/double_instance.F90 elpa-2020.05.001_ok/examples/Fortran/elpa2/double_instance.F90
--- elpa-2020.05.001/examples/Fortran/elpa2/double_instance.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa2/double_instance.F90	2020-06-25 09:22:05.776144000 +0200
@@ -0,0 +1,244 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "../assert.h"
+
+program test_interface
+   use elpa
+
+   use precision_for_tests
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE :: na, nev, nblk
+
+   ! mpi
+   TEST_INT_TYPE :: myid, nprocs
+   TEST_INT_TYPE :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_MPI_TYPE :: mpierr
+
+   ! blacs
+   TEST_INT_TYPE :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   real(kind=C_DOUBLE), allocatable :: a1(:,:), as1(:,:)
+   ! eigenvectors
+   real(kind=C_DOUBLE), allocatable :: z1(:,:)
+   ! eigenvalues
+   real(kind=C_DOUBLE), allocatable :: ev1(:)
+
+   ! The Matrix
+   complex(kind=C_DOUBLE_COMPLEX), allocatable :: a2(:,:), as2(:,:)
+   ! eigenvectors
+   complex(kind=C_DOUBLE_COMPLEX), allocatable :: z2(:,:)
+   ! eigenvalues
+   real(kind=C_DOUBLE), allocatable :: ev2(:)
+   TEST_INT_TYPE :: status
+   integer(kind=c_int) :: error_elpa
+
+   TEST_INT_TYPE :: solver
+   TEST_INT_TYPE :: qr
+
+   type(output_t) :: write_to_file
+   class(elpa_t), pointer :: e1, e2
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+   call setup_mpi(myid, nprocs)
+
+   status = 0
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+
+   np_rows = nprocs/np_cols
+
+   my_prow = mod(myid, np_cols)
+   my_pcol = myid / np_cols
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, 'C', &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a1 (na_rows,na_cols), as1(na_rows,na_cols))
+   allocate(z1 (na_rows,na_cols))
+   allocate(ev1(na))
+
+   a1(:,:) = 0.0
+   z1(:,:) = 0.0
+   ev1(:) = 0.0
+
+   call prepare_matrix_random(na, myid, sc_desc, a1, z1, as1)
+   allocate(a2 (na_rows,na_cols), as2(na_rows,na_cols))
+   allocate(z2 (na_rows,na_cols))
+   allocate(ev2(na))
+
+   a2(:,:) = 0.0
+   z2(:,:) = 0.0
+   ev2(:) = 0.0
+
+   call prepare_matrix_random(na, myid, sc_desc, a2, z2, as2)
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   e1 => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e1%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e1%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e1%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   assert(e1%setup() .eq. ELPA_OK)
+
+   call e1%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e1%set("real_kernel", ELPA_2STAGE_REAL_DEFAULT, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+
+   e2 => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e2%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e2%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+   assert(e2%setup() .eq. ELPA_OK)
+
+   call e2%set("solver", ELPA_SOLVER_1STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e1%eigenvectors(a1, ev1, z1, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_deallocate(e1, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e2%eigenvectors(a2, ev2, z2, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_deallocate(e2, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_uninit(error_elpa)
+
+   status = check_correctness_evp_numeric_residuals(na, nev, as1, z1, ev1, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+
+   deallocate(a1)
+   deallocate(as1)
+   deallocate(z1)
+   deallocate(ev1)
+
+   status = check_correctness_evp_numeric_residuals(na, nev, as2, z2, ev2, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+
+   deallocate(a2)
+   deallocate(as2)
+   deallocate(z2)
+   deallocate(ev2)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call EXIT(STATUS)
+
+
+end program
diff -ruN elpa-2020.05.001/examples/Fortran/elpa2/real_2stage_banded.F90 elpa-2020.05.001_ok/examples/Fortran/elpa2/real_2stage_banded.F90
--- elpa-2020.05.001/examples/Fortran/elpa2/real_2stage_banded.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa2/real_2stage_banded.F90	2020-06-25 09:22:05.800576000 +0200
@@ -0,0 +1,294 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+#include "../assert.h"
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 2 real case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (500, 150, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+!> The real ELPA 2 kernel is set as the default kernel.
+!> However, this can be overriden by setting
+!> the environment variable "REAL_ELPA_KERNEL" to an
+!> appropiate value.
+!>
+
+#include "config-f90.h"
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+
+program test_real2_double_banded
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - REAL version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!
+!-------------------------------------------------------------------------------
+   use elpa
+
+   !use test_util
+   use test_read_input_parameters
+   use test_check_correctness
+   use test_setup_mpi
+   use test_blacs_infrastructure
+   use test_prepare_matrix
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   use test_output_type
+   implicit none
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   TEST_INT_TYPE           :: nblk
+   TEST_INT_TYPE           :: na, nev
+
+   TEST_INT_TYPE           :: np_rows, np_cols, na_rows, na_cols
+
+   TEST_INT_TYPE           :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   TEST_INT_TYPE           :: i,  my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+   TEST_INT_MPI_TYPE       :: mpierr
+   !TEST_INT_TYPE, external :: numroc
+
+   real(kind=rk8), allocatable :: a(:,:), z(:,:), as(:,:), ev(:)
+
+   TEST_INT_TYPE           :: STATUS
+#ifdef WITH_OPENMP
+   TEST_INT_TYPE           :: omp_get_max_threads,  required_mpi_thread_level, provided_mpi_thread_level
+#endif
+   integer(kind=c_int)          :: error_elpa
+   TEST_INT_TYPE           :: numberOfDevices
+   type(output_t)             :: write_to_file
+   character(len=8)           :: task_suffix
+   TEST_INT_TYPE           :: j
+   TEST_INT_TYPE           :: global_row, global_col, local_row, local_col
+   TEST_INT_TYPE           :: bandwidth
+   class(elpa_t), pointer     :: e
+#define DOUBLE_PRECISION_REAL 1
+
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+
+   !-------------------------------------------------------------------------------
+   !  MPI Initialization
+   call setup_mpi(myid, nprocs)
+
+   STATUS = 0
+
+#define REALCASE
+
+   !-------------------------------------------------------------------------------
+   ! Selection of number of processor rows/columns
+   ! We try to set up the grid square-like, i.e. start the search for possible
+   ! divisors of nprocs with a number next to the square root of nprocs
+   ! and decrement it until a divisor is found.
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   if(myid==0) then
+      print *
+      print '(a)','Standard eigenvalue problem - REAL version'
+      print *
+      print '(3(a,i0))','Matrix size=',na,', Number of eigenvectors=',nev,', Block size=',nblk
+      print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+      print *
+   endif
+
+   !-------------------------------------------------------------------------------
+   ! Set up BLACS context and MPI communicators
+   !
+   ! The BLACS context is only necessary for using Scalapack.
+   !
+   ! For ELPA, the MPI communicators along rows/cols are sufficient,
+   ! and the grid setup may be done in an arbitrary way as long as it is
+   ! consistent (i.e. 0<=my_prow<np_rows, 0<=my_pcol<np_cols and every
+   ! process has a unique (my_prow,my_pcol) pair).
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, 'C', &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+
+   call set_up_blacs_descriptor(na ,nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+
+   !-------------------------------------------------------------------------------
+   ! Allocate matrices and set up a test matrix for the eigenvalue problem
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+
+   allocate(ev(na))
+
+   call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+
+   ! set values outside of the bandwidth to zero
+   bandwidth = nblk
+
+   do local_row = 1, na_rows
+     global_row = index_l2g(local_row, nblk, my_prow, np_rows)
+     do local_col = 1, na_cols
+       global_col = index_l2g(local_col, nblk, my_pcol, np_cols)
+
+       if (ABS(global_row-global_col) > bandwidth) then
+         a(local_row, local_col) = 0.0
+         as(local_row, local_col) = 0.0
+       end if
+     end do
+   end do
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   call e%set("bandwidth", int(bandwidth,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   assert(e%setup() .eq. ELPA_OK)
+
+   call e%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_deallocate(e, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+
+   !-------------------------------------------------------------------------------
+   ! Test correctness of result (using plain scalapack routines)
+
+   
+   status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+
+
+   deallocate(a)
+   deallocate(as)
+
+   deallocate(z)
+   deallocate(ev)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call EXIT(STATUS)
+end
+
+!-------------------------------------------------------------------------------
diff -ruN elpa-2020.05.001/examples/Fortran/elpa2/single_complex_2stage_banded.F90 elpa-2020.05.001_ok/examples/Fortran/elpa2/single_complex_2stage_banded.F90
--- elpa-2020.05.001/examples/Fortran/elpa2/single_complex_2stage_banded.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa2/single_complex_2stage_banded.F90	2020-06-25 09:22:05.782890000 +0200
@@ -0,0 +1,295 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+
+#include "../assert.h"
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 2 complex case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (500, 150, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+!> The complex ELPA 2 kernel is set as the default kernel.
+!> However, this can be overriden by setting
+!> the environment variable "COMPLEX_ELPA_KERNEL" to an
+!> appropiate value.
+!>
+program test_complex2_single_banded
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - COMPLEX version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!-------------------------------------------------------------------------------
+   use elpa
+
+   use test_util
+   use test_read_input_parameters
+   use test_check_correctness
+   use test_setup_mpi
+   use test_blacs_infrastructure
+   use test_prepare_matrix
+#ifdef HAVE_REDIRECT
+  use test_redirect
+#endif
+
+ use test_output_type
+   implicit none
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   TEST_INT_TYPE              :: nblk
+   TEST_INT_TYPE              :: na, nev
+
+   TEST_INT_TYPE              :: np_rows, np_cols, na_rows, na_cols
+
+   TEST_INT_TYPE              :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   TEST_INT_TYPE              :: i, my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+   TEST_INT_MPI_TYPE          :: mpierr
+#ifdef WITH_MPI
+   !TEST_INT_TYPE, external    :: numroc
+#endif
+   complex(kind=ck4), parameter   :: CZERO = (0.0_rk4,0.0_rk4), CONE = (1.0_rk4,0.0_rk4)
+   real(kind=rk4), allocatable    :: ev(:)
+
+   complex(kind=ck4), allocatable :: a(:,:), z(:,:), as(:,:)
+
+   TEST_INT_TYPE              :: STATUS
+#ifdef WITH_OPENMP
+   TEST_INT_TYPE              :: omp_get_max_threads,  required_mpi_thread_level, provided_mpi_thread_level
+#endif
+   type(output_t)                :: write_to_file
+   integer(kind=ik)              :: error_elpa
+   character(len=8)              :: task_suffix
+   TEST_INT_TYPE              :: j
+
+
+   TEST_INT_TYPE              :: global_row, global_col, local_row, local_col
+   TEST_INT_TYPE              :: bandwidth
+   class(elpa_t), pointer        :: e
+
+#define COMPLEXCASE
+#define DOUBLE_PRECISION_COMPLEX 1
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+      !-------------------------------------------------------------------------------
+   !  MPI Initialization
+   call setup_mpi(myid, nprocs)
+
+   STATUS = 0
+
+   !-------------------------------------------------------------------------------
+   ! Selection of number of processor rows/columns
+   ! We try to set up the grid square-like, i.e. start the search for possible
+   ! divisors of nprocs with a number next to the square root of nprocs
+   ! and decrement it until a divisor is found.
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   if(myid==0) then
+      print *
+      print '(a)','Standard eigenvalue problem - COMPLEX version'
+      print *
+      print '(3(a,i0))','Matrix size=',na,', Number of eigenvectors=',nev,', Block size=',nblk
+      print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+      print *
+   endif
+
+   !-------------------------------------------------------------------------------
+   ! Set up BLACS context and MPI communicators
+   !
+   ! The BLACS context is only necessary for using Scalapack.
+   !
+   ! For ELPA, the MPI communicators along rows/cols are sufficient,
+   ! and the grid setup may be done in an arbitrary way as long as it is
+   ! consistent (i.e. 0<=my_prow<np_rows, 0<=my_pcol<np_cols and every
+   ! process has a unique (my_prow,my_pcol) pair).
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, 'C', &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+
+   ! Determine the necessary size of the distributed matrices,
+   ! we use the Scalapack tools routine NUMROC for that.
+
+   call set_up_blacs_descriptor(na ,nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+   !-------------------------------------------------------------------------------
+   ! Allocate matrices and set up a test matrix for the eigenvalue problem
+
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+
+   allocate(ev(na))
+
+   call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+
+   ! set values outside of the bandwidth to zero
+   bandwidth = nblk
+
+   do local_row = 1, na_rows
+     global_row = index_l2g( local_row, nblk, my_prow, np_rows )
+     do local_col = 1, na_cols
+       global_col = index_l2g( local_col, nblk, my_pcol, np_cols )
+
+       if (ABS(global_row-global_col) > bandwidth) then
+         a(local_row, local_col) = 0
+         as(local_row, local_col) = 0
+       end if
+     end do
+   end do
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   call e%set("bandwidth", int(bandwidth,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   assert(e%setup() .eq. ELPA_OK)
+
+   call e%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_deallocate(e, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   !-------------------------------------------------------------------------------
+   ! Test correctness of result (using plain scalapack routines)
+   status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+
+   deallocate(a)
+   deallocate(as)
+
+   deallocate(z)
+   deallocate(ev)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call EXIT(STATUS)
+end
+
+!-------------------------------------------------------------------------------
diff -ruN elpa-2020.05.001/examples/Fortran/elpa2/single_real_2stage_banded.F90 elpa-2020.05.001_ok/examples/Fortran/elpa2/single_real_2stage_banded.F90
--- elpa-2020.05.001/examples/Fortran/elpa2/single_real_2stage_banded.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa2/single_real_2stage_banded.F90	2020-06-25 09:22:05.788644000 +0200
@@ -0,0 +1,287 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "../assert.h"
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 2 real case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (500, 150, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+!> The real ELPA 2 kernel is set as the default kernel.
+!> However, this can be overriden by setting
+!> the environment variable "REAL_ELPA_KERNEL" to an
+!> appropiate value.
+!>
+program test_real2_single_banded
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - REAL version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!
+!-------------------------------------------------------------------------------
+   use elpa
+
+   !use test_util
+   use test_read_input_parameters
+   use test_check_correctness
+   use test_setup_mpi
+   use test_blacs_infrastructure
+   use test_prepare_matrix
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   use test_output_type
+   use tests_scalapack_interfaces
+   implicit none
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   TEST_INT_TYPE           :: nblk
+   TEST_INT_TYPE           :: na, nev
+
+   TEST_INT_TYPE           :: np_rows, np_cols, na_rows, na_cols
+
+   TEST_INT_TYPE           :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   TEST_INT_TYPE           :: i, my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+   TEST_INT_MPI_TYPE       :: mpierr
+
+   real(kind=rk4), allocatable :: a(:,:), z(:,:), as(:,:), ev(:)
+
+   TEST_INT_TYPE           :: STATUS
+#ifdef WITH_OPENMP
+   TEST_INT_TYPE           :: omp_get_max_threads,  required_mpi_thread_level, provided_mpi_thread_level
+#endif
+   integer(kind=c_int)     :: error_elpa
+   type(output_t)          :: write_to_file
+   character(len=8)        :: task_suffix
+   TEST_INT_TYPE           :: j
+   TEST_INT_TYPE           :: global_row, global_col, local_row, local_col
+   TEST_INT_TYPE           :: bandwidth
+   class(elpa_t), pointer  :: e
+#define DOUBLE_PRECISION_REAL 1
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+
+   !-------------------------------------------------------------------------------
+   !  MPI Initialization
+   call setup_mpi(myid, nprocs)
+
+
+   STATUS = 0
+
+#define REALCASE
+
+   !-------------------------------------------------------------------------------
+   ! Selection of number of processor rows/columns
+   ! We try to set up the grid square-like, i.e. start the search for possible
+   ! divisors of nprocs with a number next to the square root of nprocs
+   ! and decrement it until a divisor is found.
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   if(myid==0) then
+      print *
+      print '(a)','Standard eigenvalue problem - REAL version'
+      print *
+      print '(3(a,i0))','Matrix size=',na,', Number of eigenvectors=',nev,', Block size=',nblk
+      print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+      print *
+   endif
+
+   !-------------------------------------------------------------------------------
+   ! Set up BLACS context and MPI communicators
+   !
+   ! The BLACS context is only necessary for using Scalapack.
+   !
+   ! For ELPA, the MPI communicators along rows/cols are sufficient,
+   ! and the grid setup may be done in an arbitrary way as long as it is
+   ! consistent (i.e. 0<=my_prow<np_rows, 0<=my_pcol<np_cols and every
+   ! process has a unique (my_prow,my_pcol) pair).
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, 'C', &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+
+   call set_up_blacs_descriptor(na ,nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+
+   !-------------------------------------------------------------------------------
+   ! Allocate matrices and set up a test matrix for the eigenvalue problem
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+
+   allocate(ev(na))
+
+   call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+
+   ! set values outside of the bandwidth to zero
+   bandwidth = nblk
+
+   do local_row = 1, na_rows
+     global_row = index_l2g( local_row, nblk, my_prow, np_rows )
+     do local_col = 1, na_cols
+       global_col = index_l2g( local_col, nblk, my_pcol, np_cols )
+
+       if (ABS(global_row-global_col) > bandwidth) then
+         a(local_row, local_col) = 0.0
+         as(local_row, local_col) = 0.0
+       end if
+     end do
+   end do
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   call e%set("bandwidth", int(bandwidth,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   assert(e%setup() .eq. ELPA_OK)
+
+   call e%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_deallocate(e, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+
+   !-------------------------------------------------------------------------------
+   ! Test correctness of result (using plain scalapack routines)
+
+   status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+   deallocate(a)
+   deallocate(as)
+
+   deallocate(z)
+   deallocate(ev)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call EXIT(STATUS)
+end
+
+!-------------------------------------------------------------------------------
diff -ruN elpa-2020.05.001/examples/Fortran/elpa_print_headers.F90 elpa-2020.05.001_ok/examples/Fortran/elpa_print_headers.F90
--- elpa-2020.05.001/examples/Fortran/elpa_print_headers.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/elpa_print_headers.F90	2020-06-25 09:22:05.748687000 +0200
@@ -0,0 +1,273 @@
+#if 0
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+! ELPA1 -- Faster replacements for ScaLAPACK symmetric eigenvalue routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+#endif
+
+#ifdef WITH_OPENMP
+   if (myid .eq. 0) then
+      print *,"Threaded version of test program"
+      print *,"Using ",omp_get_max_threads()," threads"
+      print *," "
+   endif
+#endif
+
+#ifndef WITH_MPI
+   if (myid .eq. 0) then
+     print *,"This version of ELPA does not support MPI parallelisation"
+     print *,"For MPI support re-build ELPA with appropiate flags"
+     print *," "
+   endif
+#endif
+
+#ifdef ELPA1
+
+#ifdef REALCASE
+#ifdef DOUBLE_PRECISION_REAL
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Real valued double-precision version of ELPA1 is used"
+     print *," "
+   endif
+#else
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Real valued single-precision version of ELPA1 is used"
+     print *," "
+   endif
+#endif
+
+#endif
+
+#ifdef COMPLEXCASE
+#ifdef DOUBLE_PRECISION_COMPLEX
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Complex valued double-precision version of ELPA1 is used"
+     print *," "
+   endif
+#else
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Complex valued single-precision version of ELPA1 is used"
+     print *," "
+   endif
+#endif
+
+#endif /* DATATYPE */
+
+#else /* ELPA1 */
+
+#ifdef REALCASE
+#ifdef DOUBLE_PRECISION_REAL
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Real valued double-precision version of ELPA2 is used"
+     print *," "
+   endif
+#else
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Real valued single-precision version of ELPA2 is used"
+     print *," "
+   endif
+#endif
+
+#endif
+
+#ifdef COMPLEXCASE
+#ifdef DOUBLE_PRECISION_COMPLEX
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Complex valued double-precision version of ELPA2 is used"
+     print *," "
+   endif
+#else
+   if (myid .eq. 0) then
+     print *," "
+     print *,"Complex valued single-precision version of ELPA2 is used"
+     print *," "
+   endif
+#endif
+
+#endif /* DATATYPE */
+
+#endif /* ELPA1 */
+
+#ifdef WITH_MPI
+    call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+#ifdef HAVE_REDIRECT
+   if (check_redirect_environment_variable()) then
+     if (myid .eq. 0) then
+       print *," "
+       print *,"Redirection of mpi processes is used"
+       print *," "
+       if (create_directories() .ne. 1) then
+         write(error_unit,*) "Unable to create directory for stdout and stderr!"
+         stop 1
+       endif
+     endif
+#ifdef WITH_MPI
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+     call redirect_stdout(myid)
+   endif
+#endif
+
+#ifndef ELPA1
+
+   if (myid .eq. 0) then
+      print *," "
+      print *,"This ELPA2 is build with"
+#ifdef WITH_GPU_KERNEL
+        print *,"GPU support"
+#endif
+      print *," "
+#ifdef REALCASE
+
+#ifdef HAVE_AVX2
+
+#ifdef WITH_REAL_AVX_BLOCK2_KERNEL
+      print *,"AVX2 optimized kernel (2 blocking) for real matrices"
+#endif
+#ifdef WITH_REAL_AVX_BLOCK4_KERNEL
+      print *,"AVX2 optimized kernel (4 blocking) for real matrices"
+#endif
+#ifdef WITH_REAL_AVX_BLOCK6_KERNEL
+      print *,"AVX2 optimized kernel (6 blocking) for real matrices"
+#endif
+
+#else /* no HAVE_AVX2 */
+
+#ifdef HAVE_AVX
+
+#ifdef WITH_REAL_AVX_BLOCK2_KERNEL
+      print *,"AVX optimized kernel (2 blocking) for real matrices"
+#endif
+#ifdef WITH_REAL_AVX_BLOCK4_KERNEL
+      print *,"AVX optimized kernel (4 blocking) for real matrices"
+#endif
+#ifdef WITH_REAL_AVX_BLOCK6_KERNEL
+      print *,"AVX optimized kernel (6 blocking) for real matrices"
+#endif
+
+#endif
+
+#endif /* HAVE_AVX2 */
+
+
+#ifdef WITH_REAL_GENERIC_KERNEL
+     print *,"GENERIC kernel for real matrices"
+#endif
+#ifdef WITH_REAL_GENERIC_SIMPLE_KERNEL
+     print *,"GENERIC SIMPLE kernel for real matrices"
+#endif
+#ifdef WITH_REAL_SSE_ASSEMBLY_KERNEL
+     print *,"SSE ASSEMBLER kernel for real matrices"
+#endif
+#ifdef WITH_REAL_BGP_KERNEL
+     print *,"BGP kernel for real matrices"
+#endif
+#ifdef WITH_REAL_BGQ_KERNEL
+     print *,"BGQ kernel for real matrices"
+#endif
+
+#endif /* DATATYPE == REAL */
+
+#ifdef COMPLEXCASE
+
+#ifdef HAVE_AVX2
+
+#ifdef  WITH_COMPLEX_AVX_BLOCK2_KERNEL
+      print *,"AVX2 optimized kernel (2 blocking) for complex matrices"
+#endif
+#ifdef WITH_COMPLEX_AVX_BLOCK1_KERNEL
+      print *,"AVX2 optimized kernel (1 blocking) for complex matrices"
+#endif
+
+#else /* no HAVE_AVX2 */
+
+#ifdef HAVE_AVX
+
+#ifdef  WITH_COMPLEX_AVX_BLOCK2_KERNEL
+      print *,"AVX optimized kernel (2 blocking) for complex matrices"
+#endif
+#ifdef WITH_COMPLEX_AVX_BLOCK1_KERNEL
+      print *,"AVX optimized kernel (1 blocking) for complex matrices"
+#endif
+
+#endif
+
+#endif /* HAVE_AVX2 */
+
+
+#ifdef WITH_COMPLEX_GENERIC_KERNEL
+     print *,"GENERIC kernel for complex matrices"
+#endif
+#ifdef WITH_COMPLEX_GENERIC_SIMPLE_KERNEL
+     print *,"GENERIC SIMPLE kernel for complex matrices"
+#endif
+#ifdef WITH_COMPLEX_SSE_ASSEMBLY_KERNEL
+     print *,"SSE ASSEMBLER kernel for complex matrices"
+#endif
+
+#endif /* DATATYPE == COMPLEX */
+
+   endif
+#endif /* ELPA1 */
+
+   if (write_to_file%eigenvectors) then
+     if (myid .eq. 0) print *,"Writing Eigenvectors to files"
+   endif
+
+   if (write_to_file%eigenvalues) then
+     if (myid .eq. 0) print *,"Writing Eigenvalues to files"
+   endif
+
+
diff -ruN elpa-2020.05.001/examples/Fortran/Makefile_examples_hybrid elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_hybrid
--- elpa-2020.05.001/examples/Fortran/Makefile_examples_hybrid	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_hybrid	2020-06-25 09:36:09.683320000 +0200
@@ -0,0 +1,25 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -qopenmp -I$(ELPA_MODULES_OPENMP) -I$(ELPA_MODULES) -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE_OPENMP)/elpa
+LIBS = -L$(ELPA_LIB) -lelpatest_openmp -lelpa_openmp $(SCALAPACK_LIB) $(MKL)
+#CC            = mpicc -O3 
+
+all: test_real_1stage_omp test_real_2stage_all_kernels_omp test_autotune_omp test_multiple_objs_omp test_split_comm_omp
+
+test_real_1stage_omp: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DTEST_EIGENVECTORS -DWITH_OPENMP -DCURRENT_API_VERSION=20190524 -DWITH_MPI -I$(ELPA_INCLUDE_OPENMP)/elpa -o test_real_1stage_omp.F90 test.F90
+	$(F90) -o $@ test_real_1stage_omp.F90 $(LIBS)
+
+test_real_2stage_all_kernels_omp: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_OPENMP -DCURRENT_API_VERSION=20190524 -DWITH_MPI -I$(ELPA_INCLUDE_OPENMP)/elpa -o test_real_2stage_all_kernels_omp.F90 test.F90
+	$(F90) -o $@ test_real_2stage_all_kernels_omp.F90 $(LIBS)
+
+test_autotune_omp: test_autotune.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DWITH_OPENMP -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_autotune.F90 $(LIBS)
+
+test_multiple_objs_omp: test_multiple_objs.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DWITH_OPENMP -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_multiple_objs.F90 $(LIBS)
+
+test_split_comm_omp: test_split_comm.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DWITH_OPENMP -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_split_comm.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/Fortran/Makefile_examples_pure elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_pure
--- elpa-2020.05.001/examples/Fortran/Makefile_examples_pure	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_pure	2020-06-25 09:36:09.684371000 +0200
@@ -0,0 +1,25 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL)
+#CC            = mpicc -O3 
+
+all: test_real_1stage test_real_2stage_all_kernels test_autotune test_multiple_objs test_split_comm
+
+test_real_1stage: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DTEST_EIGENVECTORS -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE)/elpa -o test_real_1stage.F90 test.F90
+	$(F90) -o $@ test_real_1stage.F90 $(LIBS)
+
+test_real_2stage_all_kernels: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=0 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE)/elpa -o test_real_2stage_all_kernels.F90 test.F90
+	$(F90) -o $@ test_real_2stage_all_kernels.F90 $(LIBS)
+
+test_autotune: test_autotune.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_autotune.F90 $(LIBS)
+
+test_multiple_objs: test_multiple_objs.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_multiple_objs.F90 $(LIBS)
+
+test_split_comm: test_split_comm.F90
+	$(F90) -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_split_comm.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/Fortran/Makefile_examples_pure_cuda elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_pure_cuda
--- elpa-2020.05.001/examples/Fortran/Makefile_examples_pure_cuda	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/Makefile_examples_pure_cuda	2020-06-25 09:37:52.094001000 +0200
@@ -0,0 +1,25 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB = 
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL) -lcudart
+#CC            = mpicc -O3 
+
+all: test_real_1stage test_real_2stage_all_kernels test_autotune test_multiple_objs test_split_comm
+
+test_real_1stage: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_1STAGE -DTEST_EIGENVECTORS -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE)/elpa -o test_real_1stage.F90 test.F90
+	$(F90) -o $@ test_real_1stage.F90 $(LIBS)
+
+test_real_2stage_all_kernels: test.F90
+	/usr/bin/cpp -P -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DTEST_SOLVER_2STAGE -DTEST_EIGENVECTORS -DTEST_ALL_KERNELS -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE)/elpa -o test_real_2stage_all_kernels.F90 test.F90
+	$(F90) -o $@ test_real_2stage_all_kernels.F90 $(LIBS)
+
+test_autotune: test_autotune.F90
+	$(F90) -DTEST_REAL -DTEST_GPU=1 -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_autotune.F90 $(LIBS)
+
+test_multiple_objs: test_multiple_objs.F90
+	$(F90) -DTEST_REAL -DTEST_GPU=1 -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_multiple_objs.F90 $(LIBS)
+
+test_split_comm: test_split_comm.F90
+	$(F90) -DTEST_GPU=1 -DTEST_REAL -DTEST_DOUBLE -DWITH_MPI -DCURRENT_API_VERSION=20190524 -I$(ELPA_INCLUDE_OPENMP)/elpa -o $@ test_split_comm.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/Fortran/test_autotune.F90 elpa-2020.05.001_ok/examples/Fortran/test_autotune.F90
--- elpa-2020.05.001/examples/Fortran/test_autotune.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/test_autotune.F90	2020-06-25 09:22:05.814320000 +0200
@@ -0,0 +1,312 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+! Define one of TEST_REAL or TEST_COMPLEX
+! Define one of TEST_SINGLE or TEST_DOUBLE
+! Define one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE
+! Define TEST_GPU \in [0, 1]
+! Define either TEST_ALL_KERNELS or a TEST_KERNEL \in [any valid kernel]
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+error: define exactly one of TEST_REAL or TEST_COMPLEX
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+error: define exactly one of TEST_SINGLE or TEST_DOUBLE
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE real(kind=C_FLOAT)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_FLOAT)
+#  else
+#    define MATRIX_TYPE complex(kind=C_FLOAT_COMPLEX)
+#  endif
+#else
+#  define EV_TYPE real(kind=C_DOUBLE)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_DOUBLE)
+#  else
+#    define MATRIX_TYPE complex(kind=C_DOUBLE_COMPLEX)
+#  endif
+#endif
+
+
+#ifdef TEST_REAL
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_REAL
+#else
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_COMPLEX
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "assert.h"
+
+program test
+   use elpa
+
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   use test_analytic
+   use iso_fortran_env
+
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE                     :: na, nev, nblk
+
+   ! mpi
+   TEST_INT_TYPE                     :: myid, nprocs
+   TEST_INT_TYPE                     :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE                     :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE                     :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_MPI_TYPE                 :: mpierr
+
+   ! blacs
+   character(len=1)            :: layout
+   TEST_INT_TYPE                     :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   MATRIX_TYPE, allocatable    :: a(:,:), as(:,:)
+   ! eigenvectors
+   MATRIX_TYPE, allocatable    :: z(:,:)
+   ! eigenvalues
+   EV_TYPE, allocatable        :: ev(:)
+
+   TEST_INT_TYPE               :: status
+   integer(kind=c_int)         :: error_elpa
+
+   type(output_t)              :: write_to_file
+   class(elpa_t), pointer      :: e
+   class(elpa_autotune_t), pointer :: tune_state
+
+   TEST_INT_TYPE                     :: iter
+   character(len=5)            :: iter_string
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+   call setup_mpi(myid, nprocs)
+#ifdef HAVE_REDIRECT
+#ifdef WITH_MPI
+   call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+   call redirect_stdout(myid)
+#endif
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   layout = 'C'
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   np_rows = nprocs/np_cols
+   assert(nprocs == np_rows * np_cols)
+
+   if (myid == 0) then
+     print '((a,i0))', 'Matrix size: ', na
+     print '((a,i0))', 'Num eigenvectors: ', nev
+     print '((a,i0))', 'Blocksize: ', nblk
+#ifdef WITH_MPI
+     print '((a,i0))', 'Num MPI proc: ', nprocs
+     print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+     print '(a)',      'Process layout: ' // layout
+#endif
+     print *,''
+   endif
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, layout, &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(ev(na))
+
+   a(:,:) = 0.0
+   z(:,:) = 0.0
+   ev(:) = 0.0
+
+   call prepare_matrix_analytic(na, a, nblk, myid, np_rows, np_cols, my_prow, my_pcol, print_times=.false.)
+   as(:,:) = a(:,:)
+
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if (layout .eq. 'C') then
+     call e%set("matrix_order",COLUMN_MAJOR_ORDER,error_elpa)
+   else
+     call e%set("matrix_order",ROW_MAJOR_ORDER,error_elpa)
+   endif
+
+#ifdef WITH_MPI
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+   call e%set("timings",1, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("debug",1, error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("gpu", 0, error_elpa)
+   assert_elpa_ok(error_elpa)
+   !call e%set("max_stored_rows", 15, error_elpa)
+
+   assert_elpa_ok(e%setup())
+
+   if (myid == 0) print *, ""
+
+   tune_state => e%autotune_setup(ELPA_AUTOTUNE_FAST, AUTOTUNE_DOMAIN, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   iter=0
+   do while (e%autotune_step(tune_state, error_elpa))
+     assert_elpa_ok(error_elpa)
+     iter=iter+1
+     write(iter_string,'(I5.5)') iter
+     !call e%print_settings()
+     !call e%store_settings("saved_parameters_"//trim(iter_string)//".txt")
+     call e%timer_start("eigenvectors: iteration "//trim(iter_string))
+     call e%eigenvectors(a, ev, z, error_elpa)
+     assert_elpa_ok(error_elpa)
+     call e%timer_stop("eigenvectors: iteration "//trim(iter_string))
+
+     assert_elpa_ok(error_elpa)
+     if (myid .eq. 0) then
+       print *, ""
+       call e%print_times("eigenvectors: iteration "//trim(iter_string))
+     endif
+     status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, my_prow, my_pcol, &
+                                         .true., .true., print_times=.false.)
+     a(:,:) = as(:,:)
+     !call e%autotune_print_state(tune_state)
+     !call e%autotune_save_state(tune_state, "saved_state_"//trim(iter_string)//".txt")
+   end do
+
+   ! set and print the autotuned-settings
+   call e%autotune_set_best(tune_state, error_elpa)
+   assert_elpa_ok(error_elpa)
+   if (myid .eq. 0) then
+     print *, "The best combination found by the autotuning:"
+     flush(output_unit)
+     call e%autotune_print_best(tune_state, error_elpa)
+     assert_elpa_ok(error_elpa)
+   endif
+   ! de-allocate autotune object
+   call elpa_autotune_deallocate(tune_state, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if (myid .eq. 0) then
+     print *, "Running once more time with the best found setting..."
+   endif
+   call e%timer_start("eigenvectors: best setting")
+   call e%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%timer_stop("eigenvectors: best setting")
+   assert_elpa_ok(error_elpa)
+   if (myid .eq. 0) then
+     print *, ""
+     call e%print_times("eigenvectors: best setting")
+   endif
+   status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, my_prow, my_pcol, &
+                                       .true., .true., print_times=.false.)
+
+   call elpa_deallocate(e,error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   deallocate(a)
+   deallocate(as)
+   deallocate(z)
+   deallocate(ev)
+
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+
+   call exit(status)
+
+end program
diff -ruN elpa-2020.05.001/examples/Fortran/test.F90 elpa-2020.05.001_ok/examples/Fortran/test.F90
--- elpa-2020.05.001/examples/Fortran/test.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/test.F90	2020-06-25 09:22:05.807148000 +0200
@@ -0,0 +1,915 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+! Define one of TEST_REAL or TEST_COMPLEX
+! Define one of TEST_SINGLE or TEST_DOUBLE
+! Define one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE
+! Define TEST_GPU \in [0, 1]
+! Define either TEST_ALL_KERNELS or a TEST_KERNEL \in [any valid kernel]
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+error: define exactly one of TEST_REAL or TEST_COMPLEX
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+error: define exactly one of TEST_SINGLE or TEST_DOUBLE
+#endif
+
+#if !(defined(TEST_SOLVER_1STAGE) ^ defined(TEST_SOLVER_2STAGE) ^ defined(TEST_SCALAPACK_ALL) ^ defined(TEST_SCALAPACK_PART))
+error: define exactly one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE or TEST_SCALAPACK_ALL or TEST_SCALAPACK_PART
+#endif
+
+#ifdef TEST_SOLVER_1STAGE
+#ifdef TEST_ALL_KERNELS
+error: TEST_ALL_KERNELS cannot be defined for TEST_SOLVER_1STAGE
+#endif
+#ifdef TEST_KERNEL
+error: TEST_KERNEL cannot be defined for TEST_SOLVER_1STAGE
+#endif
+#endif
+
+#ifdef TEST_SOLVER_2STAGE
+#if !(defined(TEST_KERNEL) ^ defined(TEST_ALL_KERNELS))
+error: define either TEST_ALL_KERNELS or a valid TEST_KERNEL
+#endif
+#endif
+
+#ifdef TEST_GENERALIZED_DECOMP_EIGENPROBLEM
+#define TEST_GENERALIZED_EIGENPROBLEM
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE real(kind=C_FLOAT)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_FLOAT)
+#  else
+#    define MATRIX_TYPE complex(kind=C_FLOAT_COMPLEX)
+#  endif
+#else
+#  define EV_TYPE real(kind=C_DOUBLE)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_DOUBLE)
+#  else
+#    define MATRIX_TYPE complex(kind=C_DOUBLE_COMPLEX)
+#  endif
+#endif
+
+#ifdef TEST_REAL
+#define KERNEL_KEY "real_kernel"
+#endif
+#ifdef TEST_COMPLEX
+#define KERNEL_KEY "complex_kernel"
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "assert.h"
+
+program test
+   use elpa 
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   use test_analytic
+#ifdef WITH_SCALAPACK_TESTS
+   use test_scalapack
+#endif
+
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+#ifdef WITH_OPENMP
+   use omp_lib
+#endif
+   use precision_for_tests
+
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE     :: na, nev, nblk
+
+   ! mpi
+   TEST_INT_TYPE     :: myid, nprocs
+   TEST_INT_MPI_TYPE :: myidMPI, nprocsMPI
+   TEST_INT_TYPE     :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE     :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE     :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_MPI_TYPE :: mpierr
+
+   ! blacs
+   TEST_INT_TYPE     :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   MATRIX_TYPE, allocatable    :: a(:,:), as(:,:)
+#if defined(TEST_HERMITIAN_MULTIPLY)
+   MATRIX_TYPE, allocatable    :: b(:,:), c(:,:)
+#endif
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+   MATRIX_TYPE, allocatable    :: b(:,:), bs(:,:)
+#endif
+   ! eigenvectors
+   MATRIX_TYPE, allocatable    :: z(:,:)
+   ! eigenvalues
+   EV_TYPE, allocatable        :: ev(:)
+
+   logical                     :: check_all_evals, skip_check_correctness
+
+#if defined(TEST_MATRIX_TOEPLITZ) || defined(TEST_MATRIX_FRANK)
+   EV_TYPE, allocatable        :: d(:), sd(:), ds(:), sds(:)
+   EV_TYPE                     :: diagonalELement, subdiagonalElement
+#endif
+
+   TEST_INT_TYPE               :: status
+   integer(kind=c_int)         :: error_elpa
+
+   type(output_t)              :: write_to_file
+   class(elpa_t), pointer      :: e
+#ifdef TEST_ALL_KERNELS
+   TEST_INT_TYPE      :: i
+#endif
+#ifdef TEST_ALL_LAYOUTS
+   character(len=1), parameter :: layouts(2) = [ 'C', 'R' ]
+   TEST_INT_TYPE      :: i_layout
+#endif
+   integer(kind=c_int):: kernel
+   character(len=1)   :: layout
+   logical            :: do_test_numeric_residual, do_test_numeric_residual_generalized, &
+                         do_test_analytic_eigenvalues, &
+                         do_test_analytic_eigenvalues_eigenvectors,   &
+                         do_test_frank_eigenvalues,  &
+                         do_test_toeplitz_eigenvalues, do_test_cholesky,   &
+                         do_test_hermitian_multiply
+
+#ifdef WITH_OPENMP
+   TEST_INT_TYPE      :: max_threads, threads_caller
+#endif
+
+#ifdef SPLIT_COMM_MYSELF
+   TEST_INT_MPI_TYPE  :: mpi_comm_rows, mpi_comm_cols, mpi_string_length, mpierr2
+   character(len=MPI_MAX_ERROR_STRING) :: mpierr_string
+#endif
+
+   call read_input_parameters_traditional(na, nev, nblk, write_to_file, skip_check_correctness)
+   call setup_mpi(myid, nprocs)
+
+#ifdef HAVE_REDIRECT
+#ifdef WITH_MPI
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+     call redirect_stdout(myid)
+#endif
+#endif
+
+   check_all_evals = .true.
+
+
+   do_test_numeric_residual = .false.
+   do_test_numeric_residual_generalized = .false.
+   do_test_analytic_eigenvalues = .false.
+   do_test_analytic_eigenvalues_eigenvectors = .false.
+   do_test_frank_eigenvalues = .false.
+   do_test_toeplitz_eigenvalues = .false. 
+
+   do_test_cholesky = .false.
+#if defined(TEST_CHOLESKY)
+   do_test_cholesky = .true.
+#endif
+   do_test_hermitian_multiply = .false.
+#if defined(TEST_HERMITIAN_MULTIPLY)
+   do_test_hermitian_multiply = .true.
+#endif
+
+   status = 0
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   if (myid == 0) then
+     print '((a,i0))', 'Program ' // TEST_CASE
+     print *, ""
+   endif
+
+#ifdef TEST_ALL_LAYOUTS
+   do i_layout = 1, size(layouts)               ! layouts
+     layout = layouts(i_layout)
+     do np_cols = 1, nprocs                     ! factors
+       if (mod(nprocs,np_cols) /= 0 ) then
+         cycle
+       endif
+#else
+   layout = 'C'
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+#endif
+
+   np_rows = nprocs/np_cols
+   assert(nprocs == np_rows * np_cols)
+
+   if (myid == 0) then
+     print '((a,i0))', 'Matrix size: ', na
+     print '((a,i0))', 'Num eigenvectors: ', nev
+     print '((a,i0))', 'Blocksize: ', nblk
+#ifdef WITH_MPI
+     print '((a,i0))', 'Num MPI proc: ', nprocs
+     print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+     print '(a)',      'Process layout: ' // layout
+#endif
+     print *,''
+   endif
+
+#if TEST_QR_DECOMPOSITION == 1
+
+#if TEST_GPU == 1
+#ifdef WITH_MPI
+     call mpi_finalize(mpierr)
+#endif
+     stop 77
+#endif /* TEST_GPU */
+   if (nblk .lt. 64) then
+     if (myid .eq. 0) then
+       print *,"At the moment QR decomposition need blocksize of at least 64"
+     endif
+     if ((na .lt. 64) .and. (myid .eq. 0)) then
+       print *,"This is why the matrix size must also be at least 64 or only 1 MPI task can be used"
+     endif
+
+#ifdef WITH_MPI
+     call mpi_finalize(mpierr)
+#endif
+     stop 77
+   endif
+#endif /* TEST_QR_DECOMPOSITION */
+
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, &
+                         np_cols, layout, my_blacs_ctxt, my_prow, &
+                         my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, &
+                                np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(ev(na))
+
+#ifdef TEST_HERMITIAN_MULTIPLY
+   allocate(b (na_rows,na_cols))
+   allocate(c (na_rows,na_cols))
+#endif
+
+#ifdef TEST_GENERALIZED_EIGENPROBLEM
+   allocate(b (na_rows,na_cols))
+   allocate(bs (na_rows,na_cols))
+#endif
+
+#if defined(TEST_MATRIX_TOEPLITZ) || defined(TEST_MATRIX_FRANK)
+   allocate(d (na), ds(na))
+   allocate(sd (na), sds(na))
+#endif
+
+   a(:,:) = 0.0
+   z(:,:) = 0.0
+   ev(:) = 0.0
+
+#if defined(TEST_MATRIX_RANDOM) && !defined(TEST_SOLVE_TRIDIAGONAL) && !defined(TEST_CHOLESKY) && !defined(TEST_EIGENVALUES)
+   ! the random matrix can be used in allmost all tests; but for some no
+   ! correctness checks have been implemented; do not allow these
+   ! combinations
+   ! RANDOM + TEST_SOLVE_TRIDIAGONAL: we need a TOEPLITZ MATRIX
+   ! RANDOM + TEST_CHOLESKY: wee need SPD matrix
+   ! RANDOM + TEST_EIGENVALUES: no correctness check known
+
+   ! We also have to take care of special case in TEST_EIGENVECTORS
+#if !defined(TEST_EIGENVECTORS)
+    call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+#else /* TEST_EIGENVECTORS */
+    if (nev .ge. 1) then
+      call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+#ifndef TEST_HERMITIAN_MULTIPLY
+      do_test_numeric_residual = .true.
+#endif
+   else
+     if (myid .eq. 0) then
+       print *,"At the moment with the random matrix you need nev >=1"
+     endif
+#ifdef WITH_MPI
+     call mpi_finalize(mpierr)
+#endif
+     stop 77
+   endif
+#endif /* TEST_EIGENVECTORS */
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+    do_test_frank_eigenvalues = .false.
+    do_test_toeplitz_eigenvalues = .false.
+#endif /* (TEST_MATRIX_RANDOM) */
+
+#if defined(TEST_MATRIX_RANDOM) && defined(TEST_CHOLESKY)
+     call prepare_matrix_random_spd(na, myid, sc_desc, a, z, as, &
+                 nblk, np_rows, np_cols, my_prow, my_pcol)
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+    do_test_frank_eigenvalues = .false.
+    do_test_toeplitz_eigenvalues = .false.
+#endif /* TEST_MATRIX_RANDOM and TEST_CHOLESKY */
+
+#if defined(TEST_MATRIX_RANDOM) && defined(TEST_GENERALIZED_EIGENPROBLEM)
+   ! call prepare_matrix_random(na, myid, sc_desc, a, z, as)
+    call prepare_matrix_random_spd(na, myid, sc_desc, b, z, bs, &
+                 nblk, np_rows, np_cols, my_prow, my_pcol)
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+    do_test_frank_eigenvalues = .false.
+    do_test_toeplitz_eigenvalues = .false.
+    do_test_numeric_residual = .false.
+    do_test_numeric_residual_generalized = .true.
+#endif /* TEST_MATRIX_RANDOM and TEST_GENERALIZED_EIGENPROBLEM */
+
+#if defined(TEST_MATRIX_RANDOM) && (defined(TEST_SOLVE_TRIDIAGONAL) || defined(TEST_EIGENVALUES))
+#error "Random matrix is not allowed in this configuration"
+#endif
+
+#if defined(TEST_MATRIX_ANALYTIC)  && !defined(TEST_SOLVE_TRIDIAGONAL) && !defined(TEST_CHOLESKY)
+   ! the analytic matrix can be used in allmost all tests; but for some no
+   ! correctness checks have been implemented; do not allow these
+   ! combinations
+   ! ANALYTIC + TEST_SOLVE_TRIDIAGONAL: we need a TOEPLITZ MATRIX
+   ! ANALTIC  + TEST_CHOLESKY: no correctness check yet implemented
+
+   call prepare_matrix_analytic(na, a, nblk, myid, np_rows, np_cols, my_prow, my_pcol)
+   as(:,:) = a
+
+   do_test_numeric_residual = .false.
+   do_test_analytic_eigenvalues_eigenvectors = .false.
+#ifndef TEST_HERMITIAN_MULTIPLY
+   do_test_analytic_eigenvalues = .true.
+#endif
+#if defined(TEST_EIGENVECTORS)
+   if (nev .ge. 1) then
+     do_test_analytic_eigenvalues_eigenvectors = .true.
+     do_test_analytic_eigenvalues = .false.
+   else
+     do_test_analytic_eigenvalues_eigenvectors = .false.
+   endif
+#endif
+   do_test_frank_eigenvalues = .false.
+   do_test_toeplitz_eigenvalues = .false.
+#endif /* TEST_MATRIX_ANALYTIC */
+#if defined(TEST_MATRIX_ANALYTIC) && (defined(TEST_SOLVE_TRIDIAGONAL) || defined(TEST_CHOLESKY))
+#error "Analytic matrix is not allowd in this configuration"
+#endif
+
+#if defined(TEST_MATRIX_TOEPLITZ)
+   ! The Toeplitz matrix works in each test
+#ifdef TEST_SINGLE
+   diagonalElement = 0.45_c_float
+   subdiagonalElement =  0.78_c_float
+#else
+   diagonalElement = 0.45_c_double
+   subdiagonalElement =  0.78_c_double
+#endif
+
+! actually we test cholesky for diagonal matrix only
+#if defined(TEST_CHOLESKY)
+#ifdef TEST_SINGLE
+  diagonalElement = (2.546_c_float, 0.0_c_float)
+  subdiagonalElement =  (0.0_c_float, 0.0_c_float)
+#else
+  diagonalElement = (2.546_c_double, 0.0_c_double)
+  subdiagonalElement =  (0.0_c_double, 0.0_c_double)
+#endif
+#endif /* TEST_CHOLESKY */
+
+   call prepare_matrix_toeplitz(na, diagonalElement, subdiagonalElement, &
+                                d, sd, ds, sds, a, as, nblk, np_rows, &
+                                np_cols, my_prow, my_pcol)
+
+
+   do_test_numeric_residual = .false.
+#if defined(TEST_EIGENVECTORS)
+   if (nev .ge. 1) then
+     do_test_numeric_residual = .true.
+   else
+     do_test_numeric_residual = .false.
+   endif
+#endif
+
+   do_test_analytic_eigenvalues = .false.
+   do_test_analytic_eigenvalues_eigenvectors = .false.
+   do_test_frank_eigenvalues = .false.
+#if defined(TEST_CHOLESKY)
+   do_test_toeplitz_eigenvalues = .false.
+#else
+   do_test_toeplitz_eigenvalues = .true.
+#endif
+
+#endif /* TEST_MATRIX_TOEPLITZ */
+
+
+#if defined(TEST_MATRIX_FRANK) && !defined(TEST_SOLVE_TRIDIAGONAL) && !defined(TEST_CHOLESKY)
+   ! the random matrix can be used in allmost all tests; but for some no
+   ! correctness checks have been implemented; do not allow these
+   ! combinations
+   ! FRANK + TEST_SOLVE_TRIDIAGONAL: we need a TOEPLITZ MATRIX
+   ! FRANK + TEST_CHOLESKY: no correctness check yet implemented
+
+   ! We also have to take care of special case in TEST_EIGENVECTORS
+#if !defined(TEST_EIGENVECTORS)
+    call prepare_matrix_frank(na, a, z, as, nblk, np_rows, np_cols, my_prow, my_pcol)
+
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+#ifndef TEST_HERMITIAN_MULTIPLY
+    do_test_frank_eigenvalues = .true.
+#endif
+    do_test_toeplitz_eigenvalues = .false.
+
+#else /* TEST_EIGENVECTORS */
+
+    if (nev .ge. 1) then
+      call prepare_matrix_frank(na, a, z, as, nblk, np_rows, np_cols, my_prow, my_pcol)
+
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+#ifndef TEST_HERMITIAN_MULTIPLY
+    do_test_frank_eigenvalues = .true.
+#endif
+    do_test_toeplitz_eigenvalues = .false.
+    do_test_numeric_residual = .false.
+   else
+    do_test_analytic_eigenvalues = .false.
+    do_test_analytic_eigenvalues_eigenvectors = .false.
+#ifndef TEST_HERMITIAN_MULTIPLY
+    do_test_frank_eigenvalues = .true.
+#endif
+    do_test_toeplitz_eigenvalues = .false.
+    do_test_numeric_residual = .false.
+
+   endif
+
+#endif /* TEST_EIGENVECTORS */
+#endif /* (TEST_MATRIX_FRANK) */
+#if defined(TEST_MATRIX_FRANK) && (defined(TEST_SOLVE_TRIDIAGONAL) || defined(TEST_CHOLESKY))
+#error "FRANK matrix is not allowed in this configuration"
+#endif
+
+
+#ifdef TEST_HERMITIAN_MULTIPLY
+#ifdef TEST_REAL
+
+#ifdef TEST_DOUBLE
+   b(:,:) = 2.0_c_double * a(:,:)
+   c(:,:) = 0.0_c_double
+#else
+   b(:,:) = 2.0_c_float * a(:,:)
+   c(:,:) = 0.0_c_float
+#endif
+
+#endif /* TEST_REAL */
+
+#ifdef TEST_COMPLEX
+
+#ifdef TEST_DOUBLE
+   b(:,:) = 2.0_c_double * a(:,:)
+   c(:,:) = (0.0_c_double, 0.0_c_double)
+#else
+   b(:,:) = 2.0_c_float * a(:,:)
+   c(:,:) = (0.0_c_float, 0.0_c_float)
+#endif
+
+#endif /* TEST_COMPLEX */
+
+#endif /* TEST_HERMITIAN_MULTIPLY */
+
+! if the test is used for (repeated) performacne tests, one might want to skip the checking
+! of the results, which might be time-consuming and not necessary.
+   if(skip_check_correctness) then
+     do_test_numeric_residual = .false.
+     do_test_numeric_residual_generalized = .false.
+     do_test_analytic_eigenvalues = .false.
+     do_test_analytic_eigenvalues_eigenvectors = .false.
+     do_test_frank_eigenvalues = .false.
+     do_test_toeplitz_eigenvalues = .false. 
+     do_test_cholesky = .false.
+   endif
+
+
+#ifdef WITH_OPENMP
+   threads_caller = omp_get_max_threads()
+   if (myid == 0) then
+     print *,"The calling program uses ",threads_caller," threads"
+   endif
+#endif
+
+   e => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e%set("na", int(na,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nev", int(nev,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("nblk", int(nblk,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if (layout .eq. 'C') then
+     call e%set("matrix_order",COLUMN_MAJOR_ORDER,error_elpa)
+   else
+     call e%set("matrix_order",ROW_MAJOR_ORDER,error_elpa)
+   endif
+
+#ifdef WITH_MPI
+#ifdef SPLIT_COMM_MYSELF
+   call mpi_comm_split(MPI_COMM_WORLD, int(my_pcol,kind=MPI_KIND), int(my_prow,kind=MPI_KIND), &
+                       mpi_comm_rows, mpierr)
+   if (mpierr .ne. MPI_SUCCESS) then
+     call MPI_ERROR_STRING(mpierr, mpierr_string, mpi_string_length, mpierr2)
+     write(error_unit,*) "MPI ERROR occured during mpi_comm_split for row communicator: ", trim(mpierr_string)
+     stop 1
+   endif
+
+   call mpi_comm_split(MPI_COMM_WORLD, int(my_prow,kind=MPI_KIND), int(my_pcol,kind=MPI_KIND), &
+                       mpi_comm_cols, mpierr)
+   if (mpierr .ne. MPI_SUCCESS) then
+     call MPI_ERROR_STRING(mpierr,mpierr_string, mpi_string_length, mpierr2)
+     write(error_unit,*) "MPI ERROR occured during mpi_comm_split for col communicator: ", trim(mpierr_string)
+     stop 1
+   endif
+
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("mpi_comm_rows", int(mpi_comm_rows,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("mpi_comm_cols", int(mpi_comm_cols,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#else
+   call e%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_row", int(my_prow,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+#endif
+#ifdef TEST_GENERALIZED_EIGENPROBLEM
+   call e%set("blacs_context", int(my_blacs_ctxt,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+   call e%set("timings", 1_ik, error_elpa)
+   assert_elpa_ok(e%setup())
+
+#ifdef TEST_SOLVER_1STAGE
+   call e%set("solver", ELPA_SOLVER_1STAGE, error_elpa)
+#else
+   call e%set("solver", ELPA_SOLVER_2STAGE, error_elpa)
+#endif
+   assert_elpa_ok(error_elpa)
+
+   call e%set("gpu", TEST_GPU, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+#if TEST_QR_DECOMPOSITION == 1
+   call e%set("qr", 1_ik, error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+#ifdef WITH_OPENMP
+   max_threads=omp_get_max_threads()
+   call e%set("omp_threads", int(max_threads,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+#endif
+
+   if (myid == 0) print *, ""
+
+#ifdef TEST_ALL_KERNELS
+   do i = 0, elpa_option_cardinality(KERNEL_KEY)  ! kernels
+     if (TEST_GPU .eq. 0) then
+       kernel = elpa_option_enumerate(KERNEL_KEY, int(i,kind=c_int))
+       if (kernel .eq. ELPA_2STAGE_REAL_GPU) continue
+       if (kernel .eq. ELPA_2STAGE_COMPLEX_GPU) continue
+     endif
+#endif
+#ifdef TEST_KERNEL
+     kernel = TEST_KERNEL
+#endif
+
+#ifdef TEST_SOLVER_2STAGE
+#if TEST_GPU == 1
+#if defined TEST_REAL
+     kernel = ELPA_2STAGE_REAL_GPU
+#endif
+#if defined TEST_COMPLEX
+     kernel = ELPA_2STAGE_COMPLEX_GPU
+#endif
+#endif
+     call e%set(KERNEL_KEY, kernel, error_elpa)
+#ifdef TEST_KERNEL
+     assert_elpa_ok(error_elpa)
+#else
+     if (error_elpa /= ELPA_OK) then
+       cycle
+     endif
+     ! actually used kernel might be different if forced via environment variables
+     call e%get(KERNEL_KEY, kernel, error_elpa)
+     assert_elpa_ok(error_elpa)
+#endif
+     if (myid == 0) then
+       print *, elpa_int_value_to_string(KERNEL_KEY, kernel) // " kernel"
+     endif
+#endif
+
+
+! print all parameters
+     call e%print_settings(error_elpa)
+     assert_elpa_ok(error_elpa)
+
+#ifdef TEST_ALL_KERNELS
+     call e%timer_start(elpa_int_value_to_string(KERNEL_KEY, kernel))
+#endif
+
+     ! The actual solve step
+#if defined(TEST_EIGENVECTORS)
+#if TEST_QR_DECOMPOSITION == 1
+     call e%timer_start("e%eigenvectors_qr()")
+#else
+     call e%timer_start("e%eigenvectors()")
+#endif
+#ifdef TEST_SCALAPACK_ALL
+     call solve_scalapack_all(na, a, sc_desc, ev, z)
+#elif TEST_SCALAPACK_PART
+     call solve_scalapack_part(na, a, sc_desc, nev, ev, z)
+     check_all_evals = .false. ! scalapack does not compute all eigenvectors
+#else
+     call e%eigenvectors(a, ev, z, error_elpa)
+#endif
+#if TEST_QR_DECOMPOSITION == 1
+     call e%timer_stop("e%eigenvectors_qr()")
+#else
+     call e%timer_stop("e%eigenvectors()")
+#endif
+#endif /* TEST_EIGENVECTORS  */
+
+#ifdef TEST_EIGENVALUES
+     call e%timer_start("e%eigenvalues()")
+     call e%eigenvalues(a, ev, error_elpa)
+     call e%timer_stop("e%eigenvalues()")
+#endif
+
+#if defined(TEST_SOLVE_TRIDIAGONAL)
+     call e%timer_start("e%solve_tridiagonal()")
+     call e%solve_tridiagonal(d, sd, z, error_elpa)
+     call e%timer_stop("e%solve_tridiagonal()")
+     ev(:) = d(:)
+#endif
+
+#if defined(TEST_CHOLESKY)
+     call e%timer_start("e%cholesky()")
+     call e%cholesky(a, error_elpa)
+     assert_elpa_ok(error_elpa)
+     call e%timer_stop("e%cholesky()")
+#endif
+
+#if defined(TEST_HERMITIAN_MULTIPLY)
+     call e%timer_start("e%hermitian_multiply()")
+     call e%hermitian_multiply('F','F', int(na,kind=c_int), a, b, int(na_rows,kind=c_int), &
+                               int(na_cols,kind=c_int), c, int(na_rows,kind=c_int),        &
+                               int(na_cols,kind=c_int), error_elpa)
+     call e%timer_stop("e%hermitian_multiply()")
+#endif
+
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+     call e%timer_start("e%generalized_eigenvectors()")
+#if defined(TEST_GENERALIZED_DECOMP_EIGENPROBLEM)
+     call e%timer_start("is_already_decomposed=.false.")
+#endif
+     call e%generalized_eigenvectors(a, b, ev, z, .false., error_elpa)
+#if defined(TEST_GENERALIZED_DECOMP_EIGENPROBLEM)
+     call e%timer_stop("is_already_decomposed=.false.")
+     a = as
+     call e%timer_start("is_already_decomposed=.true.")
+     call e%generalized_eigenvectors(a, b, ev, z, .true., error_elpa)
+     call e%timer_stop("is_already_decomposed=.true.")
+#endif
+     call e%timer_stop("e%generalized_eigenvectors()")
+#endif
+
+     assert_elpa_ok(error_elpa)
+
+#ifdef TEST_ALL_KERNELS
+     call e%timer_stop(elpa_int_value_to_string(KERNEL_KEY, kernel))
+#endif
+
+     if (myid .eq. 0) then
+#ifdef TEST_ALL_KERNELS
+       call e%print_times(elpa_int_value_to_string(KERNEL_KEY, kernel))
+#else /* TEST_ALL_KERNELS */
+
+#if defined(TEST_EIGENVECTORS)
+#if TEST_QR_DECOMPOSITION == 1
+       call e%print_times("e%eigenvectors_qr()")
+#else
+       call e%print_times("e%eigenvectors()")
+#endif
+#endif
+#ifdef TEST_EIGENVALUES
+       call e%print_times("e%eigenvalues()")
+#endif
+#ifdef TEST_SOLVE_TRIDIAGONAL
+       call e%print_times("e%solve_tridiagonal()")
+#endif
+#ifdef TEST_CHOLESKY
+       call e%print_times("e%cholesky()")
+#endif
+#ifdef TEST_HERMITIAN_MULTIPLY
+       call e%print_times("e%hermitian_multiply()")
+#endif
+#ifdef TEST_GENERALIZED_EIGENPROBLEM
+      call e%print_times("e%generalized_eigenvectors()")
+#endif
+#endif /* TEST_ALL_KERNELS */
+     endif
+
+     if (do_test_analytic_eigenvalues) then
+       status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, &
+                                           my_prow, my_pcol, check_all_evals, .false.)
+       call check_status(status, myid)
+     endif
+
+     if (do_test_analytic_eigenvalues_eigenvectors) then
+       status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, &
+                                           my_prow, my_pcol, check_all_evals, .true.)
+       call check_status(status, myid)
+     endif
+
+     if(do_test_numeric_residual) then
+       status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, &
+                                                        np_rows,np_cols, my_prow, my_pcol)
+       call check_status(status, myid)
+     endif
+
+     if (do_test_frank_eigenvalues) then
+       status = check_correctness_eigenvalues_frank(na, ev, z, myid)
+       call check_status(status, myid)
+     endif
+
+     if (do_test_toeplitz_eigenvalues) then
+#if defined(TEST_EIGENVALUES) || defined(TEST_SOLVE_TRIDIAGONAL)
+       status = check_correctness_eigenvalues_toeplitz(na, diagonalElement, &
+                                                       subdiagonalElement, ev, z, myid)
+       call check_status(status, myid)
+#endif
+     endif
+
+     if (do_test_cholesky) then
+       status = check_correctness_cholesky(na, a, as, na_rows, sc_desc, myid )
+       call check_status(status, myid)
+     endif
+
+#ifdef TEST_HERMITIAN_MULTIPLY
+     if (do_test_hermitian_multiply) then
+       status = check_correctness_hermitian_multiply(na, a, b, c, na_rows, sc_desc, myid )
+       call check_status(status, myid)
+     endif
+#endif
+
+#ifdef TEST_GENERALIZED_EIGENPROBLEM
+     if(do_test_numeric_residual_generalized) then
+       status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid, np_rows, &
+                                                        np_cols, my_prow, &
+       my_pcol, bs)
+       call check_status(status, myid)
+     endif
+#endif
+
+
+#ifdef WITH_OPENMP
+     if (threads_caller .ne. omp_get_max_threads()) then
+       if (myid .eq. 0) then
+         print *, " ERROR! the number of OpenMP threads has not been restored correctly"
+       endif
+       status = 1
+     endif
+#endif
+     if (myid == 0) then
+       print *, ""
+     endif
+
+#ifdef TEST_ALL_KERNELS
+     a(:,:) = as(:,:)
+#if defined(TEST_MATRIX_TOEPLITZ) || defined(TEST_MATRIX_FRANK)
+     d = ds
+     sd = sds
+#endif
+   end do ! kernels
+#endif
+
+   call elpa_deallocate(e, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   deallocate(a)
+   deallocate(as)
+   deallocate(z)
+   deallocate(ev)
+#ifdef TEST_HERMITIAN_MULTIPLY
+   deallocate(b)
+   deallocate(c)
+#endif
+#if defined(TEST_MATRIX_TOEPLITZ) || defined(TEST_MATRIX_FRANK)
+   deallocate(d, ds)
+   deallocate(sd, sds)
+#endif
+#if defined(TEST_GENERALIZED_EIGENPROBLEM)
+  deallocate(b, bs)
+#endif
+
+#ifdef TEST_ALL_LAYOUTS
+   end do ! factors
+   end do ! layouts
+#endif
+   call elpa_uninit(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+   call exit(status)
+
+   contains
+
+     subroutine check_status(status, myid)
+       implicit none
+       TEST_INT_TYPE, intent(in) :: status, myid
+       TEST_INT_MPI_TYPE         :: mpierr
+       if (status /= 0) then
+         if (myid == 0) print *, "Result incorrect!"
+#ifdef WITH_MPI
+         call mpi_finalize(mpierr)
+#endif
+         call exit(status)
+       endif
+     end subroutine
+
+end program
diff -ruN elpa-2020.05.001/examples/Fortran/test_multiple_objs.F90 elpa-2020.05.001_ok/examples/Fortran/test_multiple_objs.F90
--- elpa-2020.05.001/examples/Fortran/test_multiple_objs.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/test_multiple_objs.F90	2020-06-25 09:22:05.742700000 +0200
@@ -0,0 +1,379 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+! Define one of TEST_REAL or TEST_COMPLEX
+! Define one of TEST_SINGLE or TEST_DOUBLE
+! Define one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE
+! Define TEST_GPU \in [0, 1]
+! Define either TEST_ALL_KERNELS or a TEST_KERNEL \in [any valid kernel]
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+error: define exactly one of TEST_REAL or TEST_COMPLEX
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+error: define exactly one of TEST_SINGLE or TEST_DOUBLE
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE real(kind=C_FLOAT)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_FLOAT)
+#  else
+#    define MATRIX_TYPE complex(kind=C_FLOAT_COMPLEX)
+#  endif
+#else
+#  define EV_TYPE real(kind=C_DOUBLE)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_DOUBLE)
+#  else
+#    define MATRIX_TYPE complex(kind=C_DOUBLE_COMPLEX)
+#  endif
+#endif
+
+
+#ifdef TEST_REAL
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_REAL
+#else
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_COMPLEX
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+
+
+#include "assert.h"
+
+program test
+   use elpa
+
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   use test_analytic
+   use iso_fortran_env
+
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE                     :: na, nev, nblk
+
+   ! mpi
+   TEST_INT_TYPE                     :: myid, nprocs
+   TEST_INT_TYPE                     :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE                     :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE                     :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_TYPE                     :: ierr
+   TEST_INT_MPI_TYPE                 :: mpierr
+   ! blacs
+   character(len=1)                  :: layout
+   TEST_INT_TYPE                     :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   MATRIX_TYPE, allocatable    :: a(:,:), as(:,:)
+   ! eigenvectors
+   MATRIX_TYPE, allocatable    :: z(:,:)
+   ! eigenvalues
+   EV_TYPE, allocatable        :: ev(:)
+
+   TEST_INT_TYPE               :: status
+   integer(kind=c_int)         :: error_elpa
+
+   type(output_t)              :: write_to_file
+   class(elpa_t), pointer      :: e1, e2, e_ptr
+   class(elpa_autotune_t), pointer :: tune_state
+
+   TEST_INT_TYPE                     :: iter
+   character(len=5)            :: iter_string
+   TEST_INT_TYPE                     :: timings, debug, gpu
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+   call setup_mpi(myid, nprocs)
+#ifdef HAVE_REDIRECT
+#ifdef WITH_MPI
+   call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+   call redirect_stdout(myid)
+#endif
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   layout = 'C'
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   np_rows = nprocs/np_cols
+   assert(nprocs == np_rows * np_cols)
+
+   if (myid == 0) then
+     print '((a,i0))', 'Matrix size: ', na
+     print '((a,i0))', 'Num eigenvectors: ', nev
+     print '((a,i0))', 'Blocksize: ', nblk
+#ifdef WITH_MPI
+     print '((a,i0))', 'Num MPI proc: ', nprocs
+     print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+     print '(a)',      'Process layout: ' // layout
+#endif
+     print *,''
+   endif
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, np_cols, layout, &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(ev(na))
+
+   a(:,:) = 0.0
+   z(:,:) = 0.0
+   ev(:) = 0.0
+
+   call prepare_matrix_analytic(na, a, nblk, myid, np_rows, np_cols, my_prow, my_pcol, print_times=.false.)
+   as(:,:) = a(:,:)
+
+   e1 => elpa_allocate(error_elpa)
+   !assert_elpa_ok(error_elpa)
+
+   call set_basic_params(e1, na, nev, na_rows, na_cols, my_prow, my_pcol)
+
+   call e1%set("timings",1, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e1%set("debug",1, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e1%set("gpu", 0, error_elpa)
+   assert_elpa_ok(error_elpa)
+   !call e1%set("max_stored_rows", 15, error_elpa)
+
+   assert_elpa_ok(e1%setup())
+
+   call e1%store_settings("initial_parameters.txt", error_elpa)
+   assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+     ! barrier after store settings, file created from one MPI rank only, but loaded everywhere
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+
+   ! try to load parameters into another object
+   e2 => elpa_allocate(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call set_basic_params(e2, na, nev, na_rows, na_cols, my_prow, my_pcol)
+   call e2%load_settings("initial_parameters.txt", error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   assert_elpa_ok(e2%setup())
+
+   ! test whether the user setting of e1 are correctly loade to e2
+   call e2%get("timings", int(timings,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%get("debug", int(debug,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+   call e2%get("gpu", int(gpu,kind=c_int), error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if ((timings .ne. 1) .or. (debug .ne. 1) .or. (gpu .ne. 0)) then
+     print *, "Parameters not stored or loaded correctly. Aborting...", timings, debug, gpu
+     stop 1
+   endif
+
+   if(myid == 0) print *, "parameters of e1"
+   call e1%print_settings(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if(myid == 0) print *, ""
+   if(myid == 0) print *, "parameters of e2"
+   call e2%print_settings(error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   e_ptr => e2
+
+
+   tune_state => e_ptr%autotune_setup(ELPA_AUTOTUNE_FAST, AUTOTUNE_DOMAIN, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+
+   iter=0
+   do while (e_ptr%autotune_step(tune_state, error_elpa))
+     assert_elpa_ok(error_elpa)
+ 
+     iter=iter+1
+     write(iter_string,'(I5.5)') iter
+     call e_ptr%print_settings(error_elpa)
+     assert_elpa_ok(error_elpa)
+
+     call e_ptr%store_settings("saved_parameters_"//trim(iter_string)//".txt", error_elpa)
+     assert_elpa_ok(error_elpa)
+
+     call e_ptr%timer_start("eigenvectors: iteration "//trim(iter_string))
+     call e_ptr%eigenvectors(a, ev, z, error_elpa)
+     assert_elpa_ok(error_elpa)
+     call e_ptr%timer_stop("eigenvectors: iteration "//trim(iter_string))
+
+     assert_elpa_ok(error_elpa)
+     if (myid .eq. 0) then
+       print *, ""
+       call e_ptr%print_times("eigenvectors: iteration "//trim(iter_string))
+     endif
+     status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, my_prow, my_pcol, &
+                                         .true., .true., print_times=.false.)
+     a(:,:) = as(:,:)
+     call e_ptr%autotune_print_state(tune_state, error_elpa)
+     assert_elpa_ok(error_elpa)
+
+     call e_ptr%autotune_save_state(tune_state, "saved_state_"//trim(iter_string)//".txt", error_elpa)
+     assert_elpa_ok(error_elpa)
+#ifdef WITH_MPI
+     ! barrier after save state, file created from one MPI rank only, but loaded everywhere
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+     call e_ptr%autotune_load_state(tune_state, "saved_state_"//trim(iter_string)//".txt", error_elpa)
+     assert_elpa_ok(error_elpa)
+
+   end do
+
+   ! set and print the autotuned-settings
+   call e_ptr%autotune_set_best(tune_state, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if (myid .eq. 0) then
+     print *, "The best combination found by the autotuning:"
+     flush(output_unit)
+     call e_ptr%autotune_print_best(tune_state, error_elpa)
+     assert_elpa_ok(error_elpa)
+   endif
+   ! de-allocate autotune object
+   call elpa_autotune_deallocate(tune_state, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   if (myid .eq. 0) then
+     print *, "Running once more time with the best found setting..."
+   endif
+   call e_ptr%timer_start("eigenvectors: best setting")
+   call e_ptr%eigenvectors(a, ev, z, error_elpa)
+   assert_elpa_ok(error_elpa)
+
+   call e_ptr%timer_stop("eigenvectors: best setting")
+   assert_elpa_ok(error_elpa)
+   if (myid .eq. 0) then
+     print *, ""
+     call e_ptr%print_times("eigenvectors: best setting")
+   endif
+   status = check_correctness_analytic(na, nev, ev, z, nblk, myid, np_rows, np_cols, my_prow, my_pcol, &
+                                       .true., .true., print_times=.false.)
+
+   call elpa_deallocate(e_ptr, error_elpa)
+   !assert_elpa_ok(error_elpa)
+
+   deallocate(a)
+   deallocate(as)
+   deallocate(z)
+   deallocate(ev)
+
+   call elpa_uninit(error_elpa)
+   !assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+
+   call exit(status)
+
+contains
+   subroutine set_basic_params(elpa, na, nev, na_rows, na_cols, my_prow, my_pcol)
+     implicit none
+     class(elpa_t), pointer      :: elpa
+     TEST_INT_TYPE, intent(in)   :: na, nev, na_rows, na_cols, my_prow, my_pcol
+
+     call elpa%set("na", int(na,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nev", int(nev,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nblk", int(nblk,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+     call elpa%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_row", int(my_prow,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+#endif
+   end subroutine
+
+end program
diff -ruN elpa-2020.05.001/examples/Fortran/test_skewsymmetric.F90 elpa-2020.05.001_ok/examples/Fortran/test_skewsymmetric.F90
--- elpa-2020.05.001/examples/Fortran/test_skewsymmetric.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/test_skewsymmetric.F90	2020-06-25 09:22:05.758265000 +0200
@@ -0,0 +1,400 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+! Define one of TEST_REAL or TEST_COMPLEX
+! Define one of TEST_SINGLE or TEST_DOUBLE
+! Define one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE
+! Define TEST_GPU \in [0, 1]
+! Define either TEST_ALL_KERNELS or a TEST_KERNEL \in [any valid kernel]
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+error: define exactly one of TEST_REAL or TEST_COMPLEX
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+error: define exactly one of TEST_SINGLE or TEST_DOUBLE
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE real(kind=C_FLOAT)
+#  define EV_TYPE_COMPLEX complex(kind=C_FLOAT_COMPLEX)
+#  define MATRIX_TYPE_COMPLEX complex(kind=C_FLOAT_COMPLEX)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_FLOAT)
+#  else
+#    define MATRIX_TYPE complex(kind=C_FLOAT_COMPLEX)
+#  endif
+#else
+#  define MATRIX_TYPE_COMPLEX complex(kind=C_DOUBLE_COMPLEX)
+#  define EV_TYPE_COMPLEX complex(kind=C_DOUBLE_COMPLEX)
+#  define EV_TYPE real(kind=C_DOUBLE)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_DOUBLE)
+#  else
+#    define MATRIX_TYPE complex(kind=C_DOUBLE_COMPLEX)
+#  endif
+#endif
+
+#ifdef TEST_REAL
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_REAL
+#else
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_COMPLEX
+#endif
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "assert.h"
+
+program test
+   use elpa
+
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   use precision_for_tests
+   use iso_fortran_env
+
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE                          :: na, nev, nblk
+
+   ! mpi
+   TEST_INT_TYPE                          :: myid, nprocs
+   TEST_INT_TYPE                          :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE                          :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE                          :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_MPI_TYPE                      :: mpierr
+
+   ! blacs
+   character(len=1)                 :: layout
+   TEST_INT_TYPE                          :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   MATRIX_TYPE, allocatable         :: a_skewsymmetric(:,:), as_skewsymmetric(:,:)
+   MATRIX_TYPE_COMPLEX, allocatable :: a_complex(:,:), as_complex(:,:)
+   ! eigenvectors
+   MATRIX_TYPE, allocatable         :: z_skewsymmetric(:,:)
+   MATRIX_TYPE_COMPLEX, allocatable :: z_complex(:,:)
+   ! eigenvalues
+   EV_TYPE, allocatable             :: ev_skewsymmetric(:), ev_complex(:)
+
+   TEST_INT_TYPE                    :: status, i, j
+   integer(kind=c_int)              :: error_elpa
+
+   type(output_t)                   :: write_to_file
+   class(elpa_t), pointer           :: e_complex, e_skewsymmetric
+           
+   call read_input_parameters(na, nev, nblk, write_to_file)
+   call setup_mpi(myid, nprocs)
+#ifdef HAVE_REDIRECT
+#ifdef WITH_MPI
+   call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+   call redirect_stdout(myid)
+#endif
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+! 
+   layout = 'C'
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+      if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   np_rows = nprocs/np_cols
+   assert(nprocs == np_rows * np_cols)
+
+   if (myid == 0) then
+     print '((a,i0))', 'Matrix size: ', na
+     print '((a,i0))', 'Num eigenvectors: ', nev
+     print '((a,i0))', 'Blocksize: ', nblk
+#ifdef WITH_MPI
+     print '((a,i0))', 'Num MPI proc: ', nprocs
+     print '(3(a,i0))','Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs
+     print '(a)',      'Process layout: ' // layout
+#endif
+     print *,''
+   endif
+
+   call set_up_blacsgrid(int(mpi_comm_world,kind=BLAS_KIND), np_rows, &
+                             np_cols, layout, &
+                             my_blacs_ctxt, my_prow, my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a_skewsymmetric (na_rows,na_cols))
+   allocate(as_skewsymmetric(na_rows,na_cols))
+   allocate(z_skewsymmetric (na_rows,2*na_cols))
+   allocate(ev_skewsymmetric(na))
+
+   a_skewsymmetric(:,:) = 0.0
+   z_skewsymmetric(:,:) = 0.0
+   ev_skewsymmetric(:) = 0.0
+
+   call prepare_matrix_random(na, myid, sc_desc, a_skewsymmetric, &
+   z_skewsymmetric(:,1:na_cols), as_skewsymmetric, is_skewsymmetric=1)
+   
+   !call MPI_BARRIER(MPI_COMM_WORLD, mpierr)  
+   as_skewsymmetric(:,:) = a_skewsymmetric(:,:)
+   
+
+   ! prepare the complex matrix for the "brute force" case
+   allocate(a_complex (na_rows,na_cols))
+   allocate(as_complex(na_rows,na_cols))
+   allocate(z_complex (na_rows,na_cols))
+   allocate(ev_complex(na))
+
+   a_complex(1:na_rows,1:na_cols) = 0.0
+   z_complex(1:na_rows,1:na_cols) = 0.0
+   as_complex(1:na_rows,1:na_cols) = 0.0
+   
+
+      do j=1, na_cols
+        do i=1,na_rows
+#ifdef TEST_DOUBLE
+          a_complex(i,j) = dcmplx(0.0, a_skewsymmetric(i,j))
+#endif
+#ifdef TEST_SINGLE
+          a_complex(i,j) = cmplx(0.0, a_skewsymmetric(i,j))
+#endif
+        enddo
+      enddo
+   
+
+
+   z_complex(1:na_rows,1:na_cols)  = a_complex(1:na_rows,1:na_cols)
+   as_complex(1:na_rows,1:na_cols) = a_complex(1:na_rows,1:na_cols)
+
+   ! first set up and solve the brute force problem
+   e_complex => elpa_allocate(error_elpa)
+   call set_basic_params(e_complex, na, nev, na_rows, na_cols, my_prow, my_pcol)
+
+   call e_complex%set("timings",1, error_elpa)
+
+   call e_complex%set("debug",1,error_elpa)
+   call e_complex%set("gpu", 0,error_elpa)
+   call e_complex%set("omp_threads", 8, error_elpa)
+
+   assert_elpa_ok(e_complex%setup())
+   call e_complex%set("solver", elpa_solver_2stage, error_elpa)
+
+   call e_complex%timer_start("eigenvectors: brute force as complex matrix")
+   call e_complex%eigenvectors(a_complex, ev_complex, z_complex, error_elpa)
+   call e_complex%timer_stop("eigenvectors: brute force as complex matrix")
+
+   if (myid .eq. 0) then
+     print *, ""
+     call e_complex%print_times("eigenvectors: brute force as complex matrix")
+   endif 
+#ifdef WITH_MPI
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif     
+!      as_complex(:,:) = z_complex(:,:)
+#ifdef TEST_SINGLE
+     status = check_correctness_evp_numeric_residuals_complex_single(na, nev, as_complex, z_complex, ev_complex, sc_desc, &
+                                                    nblk, myid, np_rows,np_cols, my_prow, my_pcol)
+#else
+     status = check_correctness_evp_numeric_residuals_complex_double(na, nev, as_complex, z_complex, ev_complex, sc_desc, &
+                                                    nblk, myid, np_rows,np_cols, my_prow, my_pcol)
+#endif
+    status = 0
+    call check_status(status, myid)
+
+#ifdef WITH_MPI
+     call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+   ! now run the skewsymmetric case
+   e_skewsymmetric => elpa_allocate(error_elpa)
+   call set_basic_params(e_skewsymmetric, na, nev, na_rows, na_cols, my_prow, my_pcol)
+
+   call e_skewsymmetric%set("timings",1, error_elpa)
+
+   call e_skewsymmetric%set("debug",1,error_elpa)
+   call e_skewsymmetric%set("gpu", 0,error_elpa)
+   call e_skewsymmetric%set("omp_threads",8, error_elpa)
+
+   assert_elpa_ok(e_skewsymmetric%setup())
+   
+   call e_skewsymmetric%set("solver", elpa_solver_2stage, error_elpa)
+
+   call e_skewsymmetric%timer_start("eigenvectors: skewsymmetric ")
+   call e_skewsymmetric%skew_eigenvectors(a_skewsymmetric, ev_skewsymmetric, z_skewsymmetric, error_elpa)
+   call e_skewsymmetric%timer_stop("eigenvectors: skewsymmetric ")
+
+   if (myid .eq. 0) then
+     print *, ""
+     call e_skewsymmetric%print_times("eigenvectors: skewsymmetric")
+   endif
+   
+   ! check eigenvalues
+   do i=1, na
+     if (myid == 0) then
+#ifdef TEST_DOUBLE
+       if (abs(ev_complex(i)-ev_skewsymmetric(i))/abs(ev_complex(i)) .gt. 1e-10) then
+#endif
+#ifdef TEST_SINGLE
+       if (abs(ev_complex(i)-ev_skewsymmetric(i))/abs(ev_complex(i)) .gt. 1e-4) then
+#endif
+         print *,"ev: i=",i,ev_complex(i),ev_skewsymmetric(i)
+         status = 1
+     endif
+     endif
+   enddo
+
+
+!    call check_status(status, myid)
+   
+   z_complex(:,:) = 0
+   do j=1, na_cols
+     do i=1,na_rows
+#ifdef TEST_DOUBLE
+       z_complex(i,j) = dcmplx(z_skewsymmetric(i,j), z_skewsymmetric(i,na_cols+j))
+#endif
+#ifdef TEST_SINGLE
+       z_complex(i,j) = cmplx(z_skewsymmetric(i,j), z_skewsymmetric(i,na_cols+j))
+#endif
+     enddo
+   enddo
+#ifdef WITH_MPI
+   call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+
+#ifdef TEST_SINGLE
+   status = check_correctness_evp_numeric_residuals_ss_real_single(na, nev, as_skewsymmetric, z_complex, ev_skewsymmetric, &
+                              sc_desc, nblk, myid, np_rows,np_cols, my_prow, my_pcol)
+#else
+   status = check_correctness_evp_numeric_residuals_ss_real_double(na, nev, as_skewsymmetric, z_complex, ev_skewsymmetric, &
+                              sc_desc, nblk, myid, np_rows,np_cols, my_prow, my_pcol)
+#endif
+   
+#ifdef WITH_MPI
+    call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+#endif
+   call elpa_deallocate(e_complex,error_elpa)
+   call elpa_deallocate(e_skewsymmetric,error_elpa)
+
+
+   !to do 
+   ! - check whether brute-force check_correctness_evp_numeric_residuals worsk (complex ev)
+   ! - invent a test for skewsymmetric residuals
+
+   deallocate(a_complex)
+   deallocate(as_complex)
+   deallocate(z_complex)
+   deallocate(ev_complex)
+
+   deallocate(a_skewsymmetric)
+   deallocate(as_skewsymmetric)
+   deallocate(z_skewsymmetric)
+   deallocate(ev_skewsymmetric)
+   call elpa_uninit(error_elpa)
+
+
+
+#ifdef WITH_MPI
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+#endif
+
+   call exit(status)
+
+contains
+   subroutine set_basic_params(elpa, na, nev, na_rows, na_cols, my_prow, my_pcol)
+     implicit none
+     class(elpa_t), pointer      :: elpa
+     TEST_INT_TYPE, intent(in)         :: na, nev, na_rows, na_cols, my_prow, my_pcol
+
+     call elpa%set("na", int(na,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nev", int(nev,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nblk", int(nblk,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+
+#ifdef WITH_MPI
+     call elpa%set("mpi_comm_parent", int(MPI_COMM_WORLD,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_row", int(my_prow,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+#endif
+   end subroutine
+   subroutine check_status(status, myid)
+     implicit none
+     TEST_INT_TYPE, intent(in) :: status, myid
+     TEST_INT_MPI_TYPE         :: mpierr
+     if (status /= 0) then
+       if (myid == 0) print *, "Result incorrect!"
+#ifdef WITH_MPI
+       call mpi_finalize(mpierr)
+#endif
+       call exit(status)
+     endif
+   end subroutine
+end program
diff -ruN elpa-2020.05.001/examples/Fortran/test_split_comm.F90 elpa-2020.05.001_ok/examples/Fortran/test_split_comm.F90
--- elpa-2020.05.001/examples/Fortran/test_split_comm.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Fortran/test_split_comm.F90	2020-06-25 09:22:05.765048000 +0200
@@ -0,0 +1,340 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+#include "config-f90.h"
+
+! Define one of TEST_REAL or TEST_COMPLEX
+! Define one of TEST_SINGLE or TEST_DOUBLE
+! Define one of TEST_SOLVER_1STAGE or TEST_SOLVER_2STAGE
+! Define TEST_GPU \in [0, 1]
+! Define either TEST_ALL_KERNELS or a TEST_KERNEL \in [any valid kernel]
+
+#if !(defined(TEST_REAL) ^ defined(TEST_COMPLEX))
+error: define exactly one of TEST_REAL or TEST_COMPLEX
+#endif
+
+#if !(defined(TEST_SINGLE) ^ defined(TEST_DOUBLE))
+error: define exactly one of TEST_SINGLE or TEST_DOUBLE
+#endif
+
+#ifdef TEST_SINGLE
+#  define EV_TYPE real(kind=C_FLOAT)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_FLOAT)
+#  else
+#    define MATRIX_TYPE complex(kind=C_FLOAT_COMPLEX)
+#  endif
+#else
+#  define EV_TYPE real(kind=C_DOUBLE)
+#  ifdef TEST_REAL
+#    define MATRIX_TYPE real(kind=C_DOUBLE)
+#  else
+#    define MATRIX_TYPE complex(kind=C_DOUBLE_COMPLEX)
+#  endif
+#endif
+
+
+#ifdef TEST_REAL
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_REAL
+#else
+#  define AUTOTUNE_DOMAIN ELPA_AUTOTUNE_DOMAIN_COMPLEX
+#endif
+
+
+#ifdef HAVE_64BIT_INTEGER_MATH_SUPPORT
+#define TEST_INT_TYPE integer(kind=c_int64_t)
+#define INT_TYPE c_int64_t
+#else
+#define TEST_INT_TYPE integer(kind=c_int32_t)
+#define INT_TYPE c_int32_t
+#endif
+
+#ifdef HAVE_64BIT_INTEGER_MPI_SUPPORT
+#define TEST_INT_MPI_TYPE integer(kind=c_int64_t)
+#define INT_MPI_TYPE c_int64_t
+#else
+#define TEST_INT_MPI_TYPE integer(kind=c_int32_t)
+#define INT_MPI_TYPE c_int32_t
+#endif
+#include "assert.h"
+
+program test
+   use elpa
+
+   !use test_util
+   use test_setup_mpi
+   use test_prepare_matrix
+   use test_read_input_parameters
+   use test_blacs_infrastructure
+   use test_check_correctness
+   use test_analytic
+   use iso_fortran_env
+
+#ifdef HAVE_REDIRECT
+   use test_redirect
+#endif
+   implicit none
+
+   ! matrix dimensions
+   TEST_INT_TYPE                     :: na, nev, nblk
+   TEST_INT_TYPE                     :: num_groups, group_size, color, key
+
+   ! mpi
+   TEST_INT_TYPE                     :: myid, nprocs
+   TEST_INT_TYPE                     :: na_cols, na_rows  ! local matrix size
+   TEST_INT_TYPE                     :: np_cols, np_rows  ! number of MPI processes per column/row
+   TEST_INT_TYPE                     :: my_prow, my_pcol  ! local MPI task position (my_prow, my_pcol) in the grid (0..np_cols -1, 0..np_rows -1)
+   TEST_INT_MPI_TYPE                 :: mpierr, ierr,mpi_sub_commMPI, myidMPI, nprocsMPI, colorMPI, keyMPI, &
+                                        myid_subMPI, nprocs_subMPI
+   TEST_INT_TYPE                     :: mpi_sub_comm
+   TEST_INT_TYPE                     :: myid_sub, nprocs_sub
+
+   ! blacs
+   character(len=1)            :: layout
+   TEST_INT_TYPE                     :: my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   ! The Matrix
+   MATRIX_TYPE, allocatable    :: a(:,:), as(:,:)
+   ! eigenvectors
+   MATRIX_TYPE, allocatable    :: z(:,:)
+   ! eigenvalues
+   EV_TYPE, allocatable        :: ev(:)
+
+   TEST_INT_TYPE               :: status
+   integer(kind=c_int)         :: error_elpa
+
+   type(output_t)              :: write_to_file
+   class(elpa_t), pointer      :: e
+
+   TEST_INT_TYPE                     :: iter
+   character(len=5)            :: iter_string
+
+   status = 0
+#ifdef WITH_MPI
+
+   call read_input_parameters(na, nev, nblk, write_to_file)
+   !call setup_mpi(myid, nprocs)
+   call mpi_init(mpierr)
+   call mpi_comm_rank(mpi_comm_world, myidMPI,mpierr)
+   call mpi_comm_size(mpi_comm_world, nprocsMPI,mpierr)
+   myid = int(myidMPI,kind=BLAS_KIND)
+   nprocs = int(nprocsMPI,kind=BLAS_KIND)
+
+   if((mod(nprocs, 4) == 0) .and. (nprocs > 4)) then
+     num_groups = 4
+   else if(mod(nprocs, 3) == 0) then
+     num_groups = 3
+   else if(mod(nprocs, 2) == 0) then
+     num_groups = 2
+   else
+     num_groups = 1
+   endif
+
+   group_size = nprocs / num_groups
+
+   if(num_groups * group_size .ne. nprocs) then 
+     print *, "Something went wrong before splitting the communicator"
+     stop 1
+   else
+     if(myid == 0) then
+       print '((a,i0,a,i0))', "The test will split the global communicator into ", num_groups, " groups of size ", group_size
+     endif
+   endif
+
+   ! each group of processors will have the same color
+   color = mod(myid, num_groups)
+   ! this will determine the myid in each group
+   key = myid/num_groups
+   !split the communicator
+   colorMPI=int(color,kind=MPI_KIND)
+   keyMPI = int(key, kind=MPI_KIND)
+   call mpi_comm_split(mpi_comm_world, colorMPI, keyMPI, mpi_sub_commMPI, mpierr)
+   mpi_sub_comm = int(mpi_sub_commMPI,kind=BLAS_KIND)
+   color = int(colorMPI,kind=BLAS_KIND)
+   key = int(keyMPI,kind=BLAS_KIND)
+   if(mpierr .ne. MPI_SUCCESS) then 
+     print *, "communicator splitting not successfull", mpierr
+     stop 1
+   endif
+
+   call mpi_comm_rank(mpi_sub_commMPI, myid_subMPI, mpierr)
+   call mpi_comm_size(mpi_sub_commMPI, nprocs_subMPI, mpierr)
+   myid_sub = int(myid_subMPI,kind=BLAS_KIND)
+   nprocs_sub = int(nprocs_subMPI,kind=BLAS_KIND)
+
+   !print *, "glob ", myid, nprocs, ", loc ", myid_sub, nprocs_sub, ", color ", color, ", key ", key
+
+   if((mpierr .ne. MPI_SUCCESS) .or. (nprocs_sub .ne. group_size) .or. (myid_sub >= group_size)) then
+     print *, "something wrong with the sub communicators"
+     stop 1
+   endif
+
+
+#ifdef HAVE_REDIRECT
+   call MPI_BARRIER(MPI_COMM_WORLD, mpierr)
+   call redirect_stdout(myid)
+#endif
+
+   if (elpa_init(CURRENT_API_VERSION) /= ELPA_OK) then
+     print *, "ELPA API version not supported"
+     stop 1
+   endif
+
+   layout = 'C'
+   do np_cols = NINT(SQRT(REAL(nprocs_sub))),2,-1
+      if(mod(nprocs_sub,np_cols) == 0 ) exit
+   enddo
+   np_rows = nprocs_sub/np_cols
+   assert(nprocs_sub == np_rows * np_cols)
+   assert(nprocs == np_rows * np_cols * num_groups)
+
+   if (myid == 0) then
+     print '((a,i0))', 'Matrix size: ', na
+     print '((a,i0))', 'Num eigenvectors: ', nev
+     print '((a,i0))', 'Blocksize: ', nblk
+     print '(a)',      'Process layout: ' // layout
+     print *,''
+   endif
+   if (myid_sub == 0) then
+     print '(4(a,i0))','GROUP ', color, ': Number of processor rows=',np_rows,', cols=',np_cols,', total=',nprocs_sub
+   endif
+
+   ! USING the subcommunicator
+   call set_up_blacsgrid(int(mpi_sub_comm,kind=BLAS_KIND), np_rows, np_cols, layout, &
+                         my_blacs_ctxt, my_prow, my_pcol)
+
+   call set_up_blacs_descriptor(na, nblk, my_prow, my_pcol, np_rows, np_cols, &
+                                na_rows, na_cols, sc_desc, my_blacs_ctxt, info)
+
+   allocate(a (na_rows,na_cols))
+   allocate(as(na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+   allocate(ev(na))
+
+   a(:,:) = 0.0
+   z(:,:) = 0.0
+   ev(:) = 0.0
+
+   !call prepare_matrix_analytic(na, a, nblk, myid_sub, np_rows, np_cols, my_prow, my_pcol, print_times=.false.)
+   call prepare_matrix_random(na, myid_sub, sc_desc, a, z, as)
+   as(:,:) = a(:,:)
+
+   e => elpa_allocate(error_elpa)
+   call set_basic_params(e, na, nev, na_rows, na_cols, mpi_sub_comm, my_prow, my_pcol)
+
+   call e%set("timings",1, error_elpa)
+
+   call e%set("debug",1, error_elpa)
+   call e%set("gpu", 0, error_elpa)
+   !call e%set("max_stored_rows", 15, error_elpa)
+
+   assert_elpa_ok(e%setup())
+
+
+
+!   if(myid == 0) print *, "parameters of e"
+!   call e%print_all_parameters()
+!   if(myid == 0) print *, ""
+
+
+   call e%timer_start("eigenvectors")
+   call e%eigenvectors(a, ev, z, error_elpa)
+   call e%timer_stop("eigenvectors")
+
+   assert_elpa_ok(error_elpa)
+
+   !status = check_correctness_analytic(na, nev, ev, z, nblk, myid_sub, np_rows, np_cols, my_prow, my_pcol, &
+    !                   .true., .true., print_times=.false.)
+   status = check_correctness_evp_numeric_residuals(na, nev, as, z, ev, sc_desc, nblk, myid_sub, &
+                    np_rows,np_cols, my_prow, my_pcol)
+   if (status /= 0) &
+     print *, "processor ", myid, ": Result incorrect for processor group ", color
+
+   if (myid .eq. 0) then
+     print *, "Showing times of one goup only"
+     call e%print_times("eigenvectors")
+   endif
+
+   call elpa_deallocate(e, error_elpa)
+
+   deallocate(a)
+   deallocate(as)
+   deallocate(z)
+   deallocate(ev)
+
+   call elpa_uninit(error_elpa)
+
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+
+#endif
+   call exit(status)
+
+contains
+   subroutine set_basic_params(elpa, na, nev, na_rows, na_cols, communicator, my_prow, my_pcol)
+     use iso_c_binding
+     implicit none
+     class(elpa_t), pointer      :: elpa
+     TEST_INT_TYPE, intent(in)         :: na, nev, na_rows, na_cols, my_prow, my_pcol, communicator
+
+#ifdef WITH_MPI
+     call elpa%set("na", int(na,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nev", int(nev,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_nrows", int(na_rows,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("local_ncols", int(na_cols,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("nblk", int(nblk,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+
+     call elpa%set("mpi_comm_parent", int(communicator,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_row", int(my_prow,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+     call elpa%set("process_col", int(my_pcol,kind=c_int), error_elpa)
+     assert_elpa_ok(error_elpa)
+#endif
+   end subroutine
+
+end program
diff -ruN elpa-2020.05.001/examples/Makefile_hybrid elpa-2020.05.001_ok/examples/Makefile_hybrid
--- elpa-2020.05.001/examples/Makefile_hybrid	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Makefile_hybrid	2020-06-25 10:51:55.761200000 +0200
@@ -0,0 +1,14 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB =
+MKL = -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -qopenmp -I$(ELPA_MODULES_OPENMP) -I$(ELPA_INCLUDE_OPENMP) -I$(ELPA_INCLUDE_OPENMP)/elpa
+LIBS = -L$(ELPA_LIB_OPENMP) -lelpa_openmp -lelpatest_openmp -lelpa $(SCALAPACK_LIB) $(MKL)
+CC            = mpicc -O3 -qopenmp
+
+all: test_real_e1_omp test_real_e2_omp
+
+test_real_e1_omp: test_real_e1.F90
+	$(F90) -DWITH_OPENMP -o $@ test_real_e1.F90 $(LIBS)
+
+test_real_e2_omp: test_real_e2.F90
+	$(F90) -DWITH_OPENMP -o $@ test_real_e2.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/Makefile_pure elpa-2020.05.001_ok/examples/Makefile_pure
--- elpa-2020.05.001/examples/Makefile_pure	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Makefile_pure	2020-06-25 10:51:55.762301000 +0200
@@ -0,0 +1,14 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB =
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL)
+CC            = mpicc -O3
+
+all: test_real_e1 test_real_e2
+
+test_real_e1: test_real_e1.F90
+	$(F90) -o $@ test_real_e1.F90 $(LIBS)
+
+test_real_e2: test_real_e2.F90
+	$(F90) -o $@ test_real_e2.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/Makefile_pure_cuda elpa-2020.05.001_ok/examples/Makefile_pure_cuda
--- elpa-2020.05.001/examples/Makefile_pure_cuda	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/Makefile_pure_cuda	2020-06-25 10:51:55.763203000 +0200
@@ -0,0 +1,14 @@
+SCALAPACK_LIB = -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64
+LAPACK_LIB =
+MKL = -lmkl_intel_lp64 -lmkl_sequential -lmkl_core -liomp5 -lpthread -lstdc++
+F90 = mpif90 -O3 -I$(ELPA_MODULES) -I$(ELPA_INCLUDE) -I$(ELPA_INCLUDE)/elpa
+LIBS = -L$(ELPA_LIB) -lelpa -lelpatest $(SCALAPACK_LIB) $(MKL) -lcudart
+CC            = mpicc -O3
+
+all: test_real_e1 test_real_e2
+
+test_real_e1: test_real_e1.F90
+	$(F90) -DCUDA -o $@ test_real_e1.F90 $(LIBS)
+
+test_real_e2: test_real_e2.F90
+	$(F90) -DCUDA -o $@ test_real_e2.F90 $(LIBS)
diff -ruN elpa-2020.05.001/examples/test_real2.F90 elpa-2020.05.001_ok/examples/test_real2.F90
--- elpa-2020.05.001/examples/test_real2.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/test_real2.F90	2020-06-25 10:46:46.651243000 +0200
@@ -0,0 +1,237 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 2 real case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (4000, 1500, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+program test_real_example
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - REAL version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!
+!-------------------------------------------------------------------------------
+
+   use iso_c_binding
+
+   use elpa
+
+#ifdef HAVE_MPI_MODULE
+   use mpi
+   implicit none
+#else
+   implicit none
+   include 'mpif.h'
+#endif
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   integer           :: nblk
+   integer                          :: na, nev
+
+   integer                          :: np_rows, np_cols, na_rows, na_cols
+
+   integer                          :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   integer                          :: i, mpierr, my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   integer, external                :: numroc
+
+   real(kind=c_double), allocatable :: a(:,:), z(:,:), ev(:)
+
+   integer                          :: iseed(4096) ! Random seed, size should be sufficient for every generator
+
+   integer                          :: STATUS
+   integer                          :: success
+   character(len=8)                 :: task_suffix
+   integer                          :: j
+
+   integer, parameter               :: error_units = 0
+
+   class(elpa_t), pointer           :: e
+   !-------------------------------------------------------------------------------
+
+
+   ! default parameters
+   na = 1000
+   nev = 500
+   nblk = 16
+
+   call mpi_init(mpierr)
+   call mpi_comm_rank(mpi_comm_world,myid,mpierr)
+   call mpi_comm_size(mpi_comm_world,nprocs,mpierr)
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+     if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   ! initialise BLACS
+   my_blacs_ctxt = mpi_comm_world
+   call BLACS_Gridinit(my_blacs_ctxt, 'C', np_rows, np_cols)
+   call BLACS_Gridinfo(my_blacs_ctxt, nprow, npcol, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+   ! determine the neccessary size of the distributed matrices,
+   ! we use the scalapack tools routine NUMROC
+
+   na_rows = numroc(na, nblk, my_prow, 0, np_rows)
+   na_cols = numroc(na, nblk, my_pcol, 0, np_cols)
+
+
+   ! set up the scalapack descriptor for the checks below
+   ! For ELPA the following restrictions hold:
+   ! - block sizes in both directions must be identical (args 4 a. 5)
+   ! - first row and column of the distributed matrix must be on
+   !   row/col 0/0 (arg 6 and 7)
+
+   call descinit(sc_desc, na, na, nblk, nblk, 0, 0, my_blacs_ctxt, na_rows, info)
+
+   if (info .ne. 0) then
+     write(error_units,*) 'Error in BLACS descinit! info=',info
+     write(error_units,*) 'Most likely this happend since you want to use'
+     write(error_units,*) 'more MPI tasks than are possible for your'
+     write(error_units,*) 'problem size (matrix size and blocksize)!'
+     write(error_units,*) 'The blacsgrid can not be set up properly'
+     write(error_units,*) 'Try reducing the number of MPI tasks...'
+     call MPI_ABORT(mpi_comm_world, 1, mpierr)
+   endif
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+
+   allocate(ev(na))
+
+   ! we want different random numbers on every process
+   ! (otherwise A might get rank deficient):
+
+   iseed(:) = myid
+   call RANDOM_SEED(put=iseed)
+   call RANDOM_NUMBER(z)
+
+   a(:,:) = z(:,:)
+
+   if (myid == 0) then
+     print '(a)','| Random matrix block has been set up. (only processor 0 confirms this step)'
+   endif
+   call pdtran(na, na, 1.d0, z, 1, 1, sc_desc, 1.d0, a, 1, 1, sc_desc) ! A = A + Z**T
+
+   !-------------------------------------------------------------------------------
+
+   if (elpa_init(20171201) /= elpa_ok) then
+     print *, "ELPA API version not supported"
+     stop
+   endif
+   e => elpa_allocate()
+
+   ! set parameters decribing the matrix and it's MPI distribution
+   call e%set("na", na, success)
+   call e%set("nev", nev, success)
+   call e%set("local_nrows", na_rows, success)
+   call e%set("local_ncols", na_cols, success)
+   call e%set("nblk", nblk, success)
+   call e%set("mpi_comm_parent", mpi_comm_world, success)
+   call e%set("process_row", my_prow, success)
+   call e%set("process_col", my_pcol, success)
+
+   success = e%setup()
+
+   call e%set("solver", elpa_solver_2stage, success)
+
+
+   ! Calculate eigenvalues/eigenvectors
+
+   if (myid==0) then
+     print '(a)','| Entering two-step ELPA solver ... '
+     print *
+   end if
+
+   call mpi_barrier(mpi_comm_world, mpierr) ! for correct timings only
+   call e%eigenvectors(a, ev, z, success)
+
+   if (myid==0) then
+     print '(a)','| Two-step ELPA solver complete.'
+     print *
+   end if
+
+   call elpa_deallocate(e)
+   call elpa_uninit()
+
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+
+end
+
diff -ruN elpa-2020.05.001/examples/test_real.F90 elpa-2020.05.001_ok/examples/test_real.F90
--- elpa-2020.05.001/examples/test_real.F90	1970-01-01 01:00:00.000000000 +0100
+++ elpa-2020.05.001_ok/examples/test_real.F90	2020-06-25 10:46:26.636394000 +0200
@@ -0,0 +1,237 @@
+!    This file is part of ELPA.
+!
+!    The ELPA library was originally created by the ELPA consortium,
+!    consisting of the following organizations:
+!
+!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
+!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
+!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
+!      Informatik,
+!    - Technische Universität München, Lehrstuhl für Informatik mit
+!      Schwerpunkt Wissenschaftliches Rechnen ,
+!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
+!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
+!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
+!      and
+!    - IBM Deutschland GmbH
+!
+!
+!    More information can be found here:
+!    http://elpa.mpcdf.mpg.de/
+!
+!    ELPA is free software: you can redistribute it and/or modify
+!    it under the terms of the version 3 of the license of the
+!    GNU Lesser General Public License as published by the Free
+!    Software Foundation.
+!
+!    ELPA is distributed in the hope that it will be useful,
+!    but WITHOUT ANY WARRANTY; without even the implied warranty of
+!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+!    GNU Lesser General Public License for more details.
+!
+!    You should have received a copy of the GNU Lesser General Public License
+!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
+!
+!    ELPA reflects a substantial effort on the part of the original
+!    ELPA consortium, and we ask you to respect the spirit of the
+!    license that we chose: i.e., please contribute any changes you
+!    may have back to the original ELPA library distribution, and keep
+!    any derivatives of ELPA under the same license that we chose for
+!    the original distribution, the GNU Lesser General Public License.
+!
+!
+!>
+!> Fortran test programm to demonstrates the use of
+!> ELPA 1 real case library.
+!> If "HAVE_REDIRECT" was defined at build time
+!> the stdout and stderr output of each MPI task
+!> can be redirected to files if the environment
+!> variable "REDIRECT_ELPA_TEST_OUTPUT" is set
+!> to "true".
+!>
+!> By calling executable [arg1] [arg2] [arg3] [arg4]
+!> one can define the size (arg1), the number of
+!> Eigenvectors to compute (arg2), and the blocking (arg3).
+!> If these values are not set default values (4000, 1500, 16)
+!> are choosen.
+!> If these values are set the 4th argument can be
+!> "output", which specifies that the EV's are written to
+!> an ascii file.
+!>
+program test_real_example
+
+!-------------------------------------------------------------------------------
+! Standard eigenvalue problem - REAL version
+!
+! This program demonstrates the use of the ELPA module
+! together with standard scalapack routines
+!
+! Copyright of the original code rests with the authors inside the ELPA
+! consortium. The copyright of any additional modifications shall rest
+! with their original authors, but shall adhere to the licensing terms
+! distributed along with the original code in the file "COPYING".
+!
+!-------------------------------------------------------------------------------
+
+   use iso_c_binding
+
+   use elpa
+
+#ifdef HAVE_MPI_MODULE
+   use mpi
+   implicit none
+#else
+   implicit none
+   include 'mpif.h'
+#endif
+
+   !-------------------------------------------------------------------------------
+   ! Please set system size parameters below!
+   ! na:   System size
+   ! nev:  Number of eigenvectors to be calculated
+   ! nblk: Blocking factor in block cyclic distribution
+   !-------------------------------------------------------------------------------
+
+   integer           :: nblk
+   integer                          :: na, nev
+
+   integer                          :: np_rows, np_cols, na_rows, na_cols
+
+   integer                          :: myid, nprocs, my_prow, my_pcol, mpi_comm_rows, mpi_comm_cols
+   integer                          :: i, mpierr, my_blacs_ctxt, sc_desc(9), info, nprow, npcol
+
+   integer, external                :: numroc
+
+   real(kind=c_double), allocatable :: a(:,:), z(:,:), ev(:)
+
+   integer                          :: iseed(4096) ! Random seed, size should be sufficient for every generator
+
+   integer                          :: STATUS
+   integer                          :: success
+   character(len=8)                 :: task_suffix
+   integer                          :: j
+
+   integer, parameter               :: error_units = 0
+
+   class(elpa_t), pointer           :: e
+   !-------------------------------------------------------------------------------
+
+
+   ! default parameters
+   na = 1000
+   nev = 500
+   nblk = 16
+
+   call mpi_init(mpierr)
+   call mpi_comm_rank(mpi_comm_world,myid,mpierr)
+   call mpi_comm_size(mpi_comm_world,nprocs,mpierr)
+
+   do np_cols = NINT(SQRT(REAL(nprocs))),2,-1
+     if(mod(nprocs,np_cols) == 0 ) exit
+   enddo
+   ! at the end of the above loop, nprocs is always divisible by np_cols
+
+   np_rows = nprocs/np_cols
+
+   ! initialise BLACS
+   my_blacs_ctxt = mpi_comm_world
+   call BLACS_Gridinit(my_blacs_ctxt, 'C', np_rows, np_cols)
+   call BLACS_Gridinfo(my_blacs_ctxt, nprow, npcol, my_prow, my_pcol)
+
+   if (myid==0) then
+     print '(a)','| Past BLACS_Gridinfo.'
+   end if
+   ! determine the neccessary size of the distributed matrices,
+   ! we use the scalapack tools routine NUMROC
+
+   na_rows = numroc(na, nblk, my_prow, 0, np_rows)
+   na_cols = numroc(na, nblk, my_pcol, 0, np_cols)
+
+
+   ! set up the scalapack descriptor for the checks below
+   ! For ELPA the following restrictions hold:
+   ! - block sizes in both directions must be identical (args 4 a. 5)
+   ! - first row and column of the distributed matrix must be on
+   !   row/col 0/0 (arg 6 and 7)
+
+   call descinit(sc_desc, na, na, nblk, nblk, 0, 0, my_blacs_ctxt, na_rows, info)
+
+   if (info .ne. 0) then
+     write(error_units,*) 'Error in BLACS descinit! info=',info
+     write(error_units,*) 'Most likely this happend since you want to use'
+     write(error_units,*) 'more MPI tasks than are possible for your'
+     write(error_units,*) 'problem size (matrix size and blocksize)!'
+     write(error_units,*) 'The blacsgrid can not be set up properly'
+     write(error_units,*) 'Try reducing the number of MPI tasks...'
+     call MPI_ABORT(mpi_comm_world, 1, mpierr)
+   endif
+
+   if (myid==0) then
+     print '(a)','| Past scalapack descriptor setup.'
+   end if
+
+   allocate(a (na_rows,na_cols))
+   allocate(z (na_rows,na_cols))
+
+   allocate(ev(na))
+
+   ! we want different random numbers on every process
+   ! (otherwise A might get rank deficient):
+
+   iseed(:) = myid
+   call RANDOM_SEED(put=iseed)
+   call RANDOM_NUMBER(z)
+
+   a(:,:) = z(:,:)
+
+   if (myid == 0) then
+     print '(a)','| Random matrix block has been set up. (only processor 0 confirms this step)'
+   endif
+   call pdtran(na, na, 1.d0, z, 1, 1, sc_desc, 1.d0, a, 1, 1, sc_desc) ! A = A + Z**T
+
+   !-------------------------------------------------------------------------------
+
+   if (elpa_init(20171201) /= elpa_ok) then
+     print *, "ELPA API version not supported"
+     stop
+   endif
+   e => elpa_allocate()
+
+   ! set parameters decribing the matrix and it's MPI distribution
+   call e%set("na", na, success)
+   call e%set("nev", nev, success)
+   call e%set("local_nrows", na_rows, success)
+   call e%set("local_ncols", na_cols, success)
+   call e%set("nblk", nblk, success)
+   call e%set("mpi_comm_parent", mpi_comm_world, success)
+   call e%set("process_row", my_prow, success)
+   call e%set("process_col", my_pcol, success)
+
+   success = e%setup()
+
+   call e%set("solver", elpa_solver_1stage, success)
+
+
+   ! Calculate eigenvalues/eigenvectors
+
+   if (myid==0) then
+     print '(a)','| Entering one-step ELPA solver ... '
+     print *
+   end if
+
+   call mpi_barrier(mpi_comm_world, mpierr) ! for correct timings only
+   call e%eigenvectors(a, ev, z, success)
+
+   if (myid==0) then
+     print '(a)','| One-step ELPA solver complete.'
+     print *
+   end if
+
+   call elpa_deallocate(e)
+   call elpa_uninit()
+
+   call blacs_gridexit(my_blacs_ctxt)
+   call mpi_finalize(mpierr)
+
+end
+
