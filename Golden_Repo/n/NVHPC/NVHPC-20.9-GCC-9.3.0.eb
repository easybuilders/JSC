# This file is part of JSC's public easybuild repository (https://github.com/easybuilders/jsc)
name = 'NVHPC'
version = '20.9'
local_gccver = '9.3.0'
versionsuffix = '-GCC-%s' % local_gccver

homepage = 'https://developer.nvidia.com/hpc-sdk/'
description = """C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)"""


toolchain = SYSTEM

sources = ['nvhpc_2020_%(version_major)s%(version_minor)s_Linux_x86_64_cuda_11.0.tar.gz']
checksums = ['8fa07d762e1b48155f3d531a16b8fffeb6f28b9d8a0033a1f2ba47fdb16ffd58']

patches = ['NVHPC-20.9--nvccrc-fail.patch']

dependencies = [
    ('GCCcore', local_gccver),
    ('binutils', '2.34', '', ('GCCcore', local_gccver)),
    ('CUDA', '11.0', '', SYSTEM),
    # This is necessary to avoid cases where just libnuma.so.1 is present in the system and -lnuma fails
    ('numactl', '2.0.13', '', SYSTEM)
]

module_add_cuda = False
default_cuda_version = "11.0"
cuda_compute_capabilities = "7.0"

# We use a HMNS, so let's enforce a unique compiler
modluafooter = '''
family("compiler")
add_property("arch","gpu")
'''

# Always do a recursive unload on compilers
recursive_module_unload = True

# this bundle serves as a compiler-only toolchain, so it should be marked as compiler (important for HMNS)
moduleclass = 'compiler'
