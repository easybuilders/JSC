# This file is part of JSC's public easybuild repository (https://github.com/easybuilders/jsc)
# on juwels booster, one might to call "export UCX_LOG_LEVEL=FATAL; ebw ...
easyblock = 'PythonBundle'

name = 'Horovod'
version = '0.25.0'
local_tf_version = '2.6.0'

homepage = 'https://github.com/uber/horovod'
description = "Horovod is a distributed training framework for TensorFlow and PyTorch."

toolchain = {'name': 'gomkl', 'version': '2021b'}
toolchainopts = {'pic': True}

builddependencies = [
    ('CMake', '3.21.1', '', SYSTEM),
]

dependencies = [
    ('CUDA', '11.5', '', True),
    ('Python', '3.9.6'),
    ('TensorFlow', local_tf_version, '-CUDA-%(cudaver)s', ('gcccoremkl', '11.2.0-2021.4.0')),
    ('PyTorch', '1.11', '-CUDA-%(cudaver)s', ('gcccoremkl', '11.2.0-2021.4.0')),
    ('NCCL', '2.14.3-1', '-CUDA-%(cudaver)s'),
]

use_pip = True
sanity_pip_check = True
parallel = 1  # Bug in CMake causes a race condition on horovod_cuda_kernels_generated_cuda_kernels.cu.o.NVCC-depend

# possible vars:
# HOROVOD_BUILD_ARCH_FLAGS - additional C++ compilation flags to pass in for your build architecture.
# HOROVOD_CUDA_HOME - path where CUDA include and lib directories can be found.
# HOROVOD_BUILD_CUDA_CC_LIST - List of compute capabilities to build Horovod CUDA
# kernels for (example: HOROVOD_BUILD_CUDA_CC_LIST=60,70,75)
# HOROVOD_ROCM_HOME - path where ROCm include and lib directories can be found.
# HOROVOD_NCCL_HOME - path where NCCL include and lib directories can be found.
# HOROVOD_NCCL_INCLUDE - path to NCCL include directory.
# HOROVOD_NCCL_LIB - path to NCCL lib directory.
# HOROVOD_NCCL_LINK - {SHARED, STATIC}. Mode to link NCCL library. Defaults to STATIC for CUDA, SHARED for ROCm.
# HOROVOD_WITH_GLOO - {1}. Require that Horovod is built with Gloo support enabled.
# HOROVOD_WITHOUT_GLOO - {1}. Skip building with Gloo support.
# HOROVOD_WITH_MPI - {1}. Require that Horovod is built with MPI support enabled.
# HOROVOD_WITHOUT_MPI - {1}. Skip building with MPI support.
# HOROVOD_GPU - {CUDA, ROCM}. Framework to use for GPU operations.
# HOROVOD_GPU_OPERATIONS - {NCCL, MPI}. Framework to use for GPU tensor allreduce, allgather, and broadcast.
# HOROVOD_GPU_ALLREDUCE - {NCCL, MPI}. Framework to use for GPU tensor allreduce.
# HOROVOD_GPU_ALLGATHER - {NCCL, MPI}. Framework to use for GPU tensor allgather.
# HOROVOD_GPU_BROADCAST - {NCCL, MPI}. Framework to use for GPU tensor broadcast.
# HOROVOD_ALLOW_MIXED_GPU_IMPL - {1}. Allow Horovod to install with NCCL allreduce and MPI GPU allgather /
# broadcast. Not recommended due to a possible deadlock.
# HOROVOD_CPU_OPERATIONS - {MPI, GLOO, CCL}. Framework to use for CPU tensor allreduce, allgather, and broadcast.
# HOROVOD_CMAKE - path to the CMake binary used to build Gloo (not required when using MPI).
# HOROVOD_WITH_TENSORFLOW - {1}. Require Horovod to install with TensorFlow support enabled.
# HOROVOD_WITHOUT_TENSORFLOW - {1}. Skip installing TensorFlow support.
# HOROVOD_WITH_PYTORCH - {1}. Require Horovod to install with PyTorch support enabled.
# HOROVOD_WITHOUT_PYTORCH - {1}. Skip installing PyTorch support.
# HOROVOD_WITH_MXNET - {1}. Require Horovod to install with MXNet support enabled.
# HOROVOD_WITHOUT_MXNET - {1}. Skip installing MXNet support.

# prebuildopts = 'export LDSHARED="$CC -shared" && '
# prebuildopts += ' HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITHOUT_PYTORCH=1 HOROVOD_WITHOUT_MXNET=1 '
# prebuildopts += ' HOROVOD_NCCL_LINK=SHARED HOROVOD_NCCL_HOME=$EBROOTNCCL '
# prebuildopts += ' HOROVOD_GPU_OPERATIONS=NCCL '
# prebuildopts += ' HOROVOD_CPU_OPERATIONS=MPI '
# prebuildopts += ' HOROVOD_GPU_ALLREDUCE=NCCL '
# prebuildopts += ' HOROVOD_GPU_BROADCAST=NCCL '
# prebuildopts += ' HOROVOD_WITH_MPI=1 '


prebuildopts = 'export LDSHARED="$CC -shared" && '
prebuildopts += ' HOROVOD_WITH_MPI=1 '
prebuildopts += ' HOROVOD_CPU_OPERATIONS=MPI '
prebuildopts += ' HOROVOD_NCCL_LINK=SHARED HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_NCCL_HOME=$EBROOTNCCL '
prebuildopts += ' HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1'
# prebuildopts += ' NVCC_GENCODE="-gencode=arch=compute_70,code=sm_70 \
#                                -gencode=arch=compute_75,code=sm_75 \
#                                -gencode=arch=compute_80,code=sm_80 \
#                                -gencode=arch=compute_86,code=sm_86"'

preinstallopts = prebuildopts


exts_default_options = {'source_urls': [PYPI_SOURCE]}

exts_list = [
    ('horovod', version, {
        'checksums': ['bc9fed57b67c1b55259671d2439cdbc93aa897ea6e5da459e11e7556972b2355'],
    }),
]

sanity_check_paths = {
    'files': ['bin/horovodrun'],
    'dirs': ['lib/python%(pyshortver)s/site-packages'],
}

# This makes openmpi work. It's up to the sysadmins to correct me here.
modextravars = {'HOROVOD_MPI_THREADS_DISABLE': '1'}

modloadmsg = 'Setting HOROVOD_MPI_THREADS_DISABLE=1. '

moduleclass = 'tools'
